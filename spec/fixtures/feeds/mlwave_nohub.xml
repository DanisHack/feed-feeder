<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>MLWave</title>
	<atom:link href="http://mlwave.com/feed/" rel="self" type="application/rss+xml" />
	<link>http://mlwave.com</link>
	<description>Learning Machine Learning</description>
	<lastBuildDate>Thu, 03 Dec 2015 18:08:57 +0000</lastBuildDate>
	<language>en-US</language>
		<sy:updatePeriod>hourly</sy:updatePeriod>
		<sy:updateFrequency>1</sy:updateFrequency>
	<generator>http://wordpress.org/?v=3.9.11</generator>
	<item>
		<title>How we won 3rd Prize in CrowdAnalytix COPD competition</title>
		<link>http://mlwave.com/how-we-won-3rd-prize-in-crowdanalytix-copd-competition/</link>
		<comments>http://mlwave.com/how-we-won-3rd-prize-in-crowdanalytix-copd-competition/#comments</comments>
		<pubDate>Mon, 14 Sep 2015 11:30:11 +0000</pubDate>
		<dc:creator><![CDATA[mladmin]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://mlwave.com/?p=1095</guid>
		<description><![CDATA[I recently competed in a CrowdAnalytix competition to predict worsening symptoms of COPD. Our team (Marios Michailidis, Phil Culliton, Vivant Shen and me) finished in the money. Here is how we managed this. COPD COPD (Chronic Obstructive Pulmonary Disease) is a lung disease that makes it hard to breath. People with COPD experience exacerbation: A sudden worsening of [&#8230;]]]></description>
				<content:encoded><![CDATA[<p><strong>I recently competed in a CrowdAnalytix competition to predict worsening symptoms of COPD. Our team (Marios Michailidis, Phil Culliton, Vivant Shen and me) finished in the money. Here is how we managed this.</strong></p>
<h2>COPD</h2>
<p>COPD (Chronic Obstructive Pulmonary Disease) is a lung disease that makes it hard to breath. People with COPD experience exacerbation: A sudden worsening of the symptoms. <a href="http://www.webmd.com/lung/10-signs-copd-exacerbation">Symptoms</a> of COPD exacerbation include:</p>
<ul>
<li>Shortness of breath</li>
<li>Noisy or irregular breathing</li>
<li>Worry and muscle tension</li>
<li>Trouble getting to sleep</li>
<li>Swollen ankles</li>
</ul>
<p><span id="more-1095"></span></p>
<h2>Predicting Exacerbation</h2>
<p>Our task in this <a href="https://crowdanalytix.com/">CrowdAnalytix</a> competition is to <a href="https://crowdanalytix.com/contests/predict-exacerbation-in-patients-with-copd">predict whether COPD patients</a> will show an onset of exacerbation.</p>
<p>Knowing that a patient will have exacerbation can aid medical professionals in making better, more informed, choices.<br />
For instance, <a href="http://www.healthline.com/health/copd/drugs">medication</a> can help reduce inflammation, possibly shortening or easing the exacerbation.</p>
<p>A solution for this problem could be part of a more general decision making tool.</p>
<h2>The data</h2>
<p>To predict COPD exacerbation we have a small dataset. Small datasets are common with medical datasets: gathering data from different patients over prolonged periods is a lot of work. Also more data may simply not be available.</p>
<blockquote><p>There are two types of machine learning practitioner:</p>
<p>1) Those who can generalise from limited data.<br />
&#8211; <cite><a href="https://twitter.com/ml_hipster">@ML_Hipster</a></cite></p></blockquote>
<p>Every observation is a patient in the study. A single patient only appears in the train or test set, but never in both. This ensures that solutions generalize to other patients: we train on different patients than the patients we are making predictions for.</p>
<p>The data is anonymized so as to protect the privacy of the participating patients (3309 in total).</p>
<h2>The features</h2>
<p>We have 60 features of varying usefulness, including:</p>
<ul>
<li>Patient demographics</li>
<li>Disease stage</li>
<li>Lung function</li>
<li>Disease history</li>
<li>Questionnaire results</li>
<li>Smoking history</li>
</ul>
<p>Showing which variables were important was required to be eligible for a prize.</p>
<h2>Plan of action</h2>
<p>We divvied up the task so we could attack the problem from multiple angles.</p>
<ul>
<li><a href="https://www.linkedin.com/in/mariosmichailidis/">Marios Michailidis</a>: Forests and Linear/Logistic Regression</li>
<li><a href="https://www.linkedin.com/in/PhilCulliton/">Phil Culliton</a>: Neural networks (ANN, MLP, DL)</li>
<li><a href="https://www.linkedin.com/pub/vivant-shen/3b/681/3b8">Vivant Shen</a>: XGBoost and Scikit-learn <a href="http://scikit-learn.org/stable/modules/ensemble.html">.ensemble</a></li>
<li><a href="https://www.linkedin.com/in/hjvanveen">HJ van Veen</a>: <a href="https://github.com/JohnLangford/vowpal_wabbit/wiki">Vowpal Wabbit</a> and Hodor-ML (Automated stacking with many different algorithms)</li>
</ul>
<h2>Benchmark</h2>
<p>Marios Michailidis created our team benchmark/baseline. This was a single Logistic Regression model which showed solid cross-validation and a decent score.</p>
<p>Such linear benchmarks are useful to:</p>
<ul>
<li>spot good features,</li>
<li>tell you if the problem is more linear or non-linear</li>
<li>show you if a model is worth improving on or if it should be discarded</li>
</ul>
<p>None of our models showed much promise vs. this baseline, except for a very basic <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7485&amp;rep=rep1&amp;type=pdf">Extremely Randomized Trees</a> model from Vivant Shen. She was the only one to experiment with feature selection. This turned out to be a winning intuition.</p>
<h2>Extremely Randomized Trees</h2>
<p>Extremely Randomized Trees is inspired by Perfect Random Trees. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.232.2940&amp;rep=rep1&amp;type=pdf">Perfect Random Tree Ensembles</a> create random trees that continue to grow until all leafs have a 100% ratio of the same class.</p>
<p>Extremely Randomized Trees is very similar to the Perfect Random Trees principle and the Random Forests algorithm. It creates trees which are split more randomly, in the extreme case building completely randomly split trees.</p>
<p>It depends on the problem/data if <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html">ExtraTrees</a> outperforms RandomForests. In my practical experience with this algorithm it does so more often than not (provided you can use a large amount of estimators), making ExtraTrees a solid choice for a wide variety of dense data sets.</p>
<p>ExtraTrees is also not too popular an algorithm (compared to, say, <a href="https://github.com/dmlc/xgboost">XGBoost</a>). This may provide you with an edge over other competitors using more popular algorithms.</p>
<p>The added element of randomness makes ExtraTrees more robust against overfit: While individual trees may be highly variant, the trees themselves are much less correlated to another. Ensembles of trees <a href="http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf">reduce variance</a> and using less correlated trees <a href="http://www.complex-systems.com/abstracts/v04_i02_a04.html">reduces the generalization error</a>.</p>
<h2>Feature Selection</h2>
<p>Vivant noticed that halving the dimensionality improved her results.</p>
<p>With tools like Vowpal Wabbit adding a few uninformative features will not damage your results too much. Tree-based models really do benefit by removing these noisy features. If kept in they are sure to appear in a few trees, muddying the results.</p>
<p>Eliminating even more features, to just the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html">top 25 best features</a> gave us the model with the best score.</p>
<blockquote><p>&#8220;It seems that perfection is reached not when there is nothing left to add, but when there is nothing left to take away&#8221; &#8211; <cite>Antoine de Saint-Exupéry</cite></p></blockquote>
<h2>Bagging</h2>
<p>The score was made more solid by averaging the result of 10 ExtraTrees models using a different seed/random state.</p>
<p>This modest modeling approach I completely overlooked. I had expected that stacked generalization or at least averaging with different algorithms would help. This served as a warning to never ignore the basics.</p>
<p>It seems the <a href="https://en.wikipedia.org/wiki/KISS_principle">KISS-principle</a> does apply to data science: You should never underestimate simplicity and elegance, especially when it beats your complex approaches.</p>
<h2>About the competition</h2>
<p>What I liked about this competition was the way prize money was awarded: Even though we ended up in the 7th spot, we still won 3rd prize. This rewards all the well-performing teams, resulting in more diverse model documentation, less statistically insignificant winners, and increased engagement and satisfaction.</p>
<p>I did not like that our best model was picked automatically. This favors using a shotgun approach over looking at CV and carefully selecting your final models. Though it is an improvement over the previous competitions, where there was no private leaderboard at all (thus rewarding overfitting to the leaderboard).</p>
<p>If their software allows for this, I&#8217;d recommend that CrowdAnalytix have us select our final models.</p>
<h2>Conclusion</h2>
<blockquote><p>Medical diagnostics is, at its heart, a data problem &#8211; turning images, lab tests, patient histories, and so forth into a diagnosis and proposed intervention. Recent applied machine learning breakthroughs, especially using deep learning, have shown that computers can rapidly turn large amounts of data of this kind into deep insights, and find subtle patterns. This is the biggest opportunity for positive impact using data that I&#8217;ve seen in my 20+ years in the field. <cite><a href="https://twitter.com/jeremyphoward">Jeremy Howard</a></cite></p></blockquote>
<p>The simple, modest and solid approach won vs. overkill solutions. Bigger is not always better. Elegance ruled over brute-force. This is a good thing: Our solution would have little problems in real-life implementation. An ensemble of 1000s of models may have won this competition, but it would not be very useful. <a href="https://vimeo.com/125940125">Model interpretability</a> is very important for medical models.</p>
<p>In general, it would be nice if more data like this is made available. More and better data can aid research and removes uncertainty. A lot of this data is already out there, hidden in the databases of different hospitals.</p>
<p>Combining this information (in a way like <a href="http://www.enlitic.com/">Enlitic</a> is doing for medical photography) is valuable. It should be easier for patients to give permission: <em>&#8220;Yes! Use my (anonymized) data for science!&#8221;</em>. Besides an improved technical infrastructure such sharing initiatives need a change in mindset for all professionals (and patients) involved.</p>
<p>We each picked a good cause and donated the prize money to:</p>
<ul>
<li><a href="https://www.blf.org.uk/Home">British Lung Foundation</a></li>
<li><a href="http://www.copdfoundation.org/">COPD Foundation</a></li>
<li><a href="http://www.internationalanimalrescue.org/">International Animal Rescue</a></li>
<li><a href="https://secure.savethechildren.org.uk/donate/">Save the Children</a></li>
</ul>
<p>Thanks to my team mates, CrowdAnalytix, competition host Mohan S, <a href="http://www.montefiore.ulg.ac.be/~geurts/">Pierre Geurts</a> et al., and everyone contributing to Scikit-learn.</p>
<p><small>The intro image for this post came from WikiMedia Commons and is in the Public Domain, courtesy of the medical illustrator <a href="https://en.wikipedia.org/wiki/Patrick_J._Lynch">Patrick J. Lynch</a> and uploaded by <a href="https://commons.wikimedia.org/wiki/User:Materialscientist">MaterialScientist</a>.</small></p>
]]></content:encoded>
			<wfw:commentRss>http://mlwave.com/how-we-won-3rd-prize-in-crowdanalytix-copd-competition/feed/</wfw:commentRss>
		<slash:comments>8</slash:comments>
		</item>
		<item>
		<title>Kaggle Ensembling Guide</title>
		<link>http://mlwave.com/kaggle-ensembling-guide/</link>
		<comments>http://mlwave.com/kaggle-ensembling-guide/#comments</comments>
		<pubDate>Thu, 11 Jun 2015 20:39:38 +0000</pubDate>
		<dc:creator><![CDATA[mladmin]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://mlwave.com/?p=877</guid>
		<description><![CDATA[Model ensembling is a very powerful technique to increase accuracy on a variety of ML tasks. In this article I will share my ensembling approaches for Kaggle Competitions. For the first part we look at creating ensembles from submission files. The second part will look at creating ensembles through stacked generalization/blending. I answer why ensembling [&#8230;]]]></description>
				<content:encoded><![CDATA[<p><strong>Model ensembling is a very powerful technique to increase accuracy on a variety of ML tasks. In this article I will share my ensembling approaches for Kaggle Competitions.</strong></p>
<p>For the first part we look at creating ensembles from submission files. The second part will look at creating ensembles through stacked generalization/blending.</p>
<p>I answer why ensembling reduces the generalization error. Finally I show different methods of ensembling, together with their results and code to try it out for yourself.</p>
<blockquote><p>This is how you win ML competitions: you take other peoples&#8217; work and ensemble them together.&#8221; <cite><a href="http://cims.nyu.edu/~vitaly/">Vitaly Kuznetsov</a> NIPS2014</cite></p></blockquote>
<p><span id="more-877"></span></p>
<h2>Creating ensembles from submission files</h2>
<p>The most basic and convenient way to ensemble is to ensemble Kaggle submission CSV files. You only need the predictions on the test set for these methods &#8212; no need to retrain a model. This makes it a quick way to ensemble already existing model predictions, ideal when teaming up.</p>
<h3>Voting ensembles.</h3>
<p>We first take a look at a simple majority vote ensemble. Let&#8217;s see why model ensembling reduces error rate and why it works better to ensemble low-correlated model predictions.</p>
<h4>Error correcting codes</h4>
<p>During space missions it is very important that all signals are correctly relayed.</p>
<p>If we have a signal in the form of a binary string like:</p>
<pre>1<strong>1</strong>10110011101111011111011011
</pre>
<p>and somehow this signal is corrupted (a bit is flipped) to:</p>
<pre>1<b>0</b>10110011101111011111011011
</pre>
<p>then lives could be lost.</p>
<p>A <a href="http://en.wikipedia.org/wiki/Coding_theory">coding</a> solution was found in <a href="http://en.wikipedia.org/wiki/Forward_error_correction">error correcting codes</a>. The simplest error correcting code is a <a href="http://en.wikipedia.org/wiki/Repetition_code">repetition-code</a>: Relay the signal multiple times in equally sized chunks and have a majority vote.</p>
<pre>Original signal:
1110110011

Encoded:
10,3 101011001111101100111110110011

Decoding:
1<b>0</b>10110011
1<b>1</b>10110011
1<b>1</b>10110011

Majority vote:
1<b>1</b>10110011
</pre>
<p>Signal corruption is a very rare occurrence and often occur in small bursts. So then it figures that it is even rarer to have a corrupted majority vote.</p>
<p>As long as the corruption is not completely unpredictable (has a 50% chance of occurring) then signals can be repaired.</p>
<h4>A machine learning example</h4>
<p>Suppose we have a test set of 10 samples. The ground truth is all positive (&#8220;1&#8243;):</p>
<pre>1111111111
</pre>
<p>We furthermore have 3 binary classifiers (A,B,C) with a 70% accuracy. You can view these classifiers for now as pseudo-random number generators which output a &#8220;1&#8243; 70% of the time and a &#8220;0&#8243; 30% of the time.</p>
<p>We will now show how these pseudo-classifiers are able to obtain 78% accuracy through a voting ensemble.</p>
<h5>A pinch of maths</h5>
<p>For a majority vote with 3 members we can expect 4 outcomes:</p>
<pre>All three are correct
  0.7 * 0.7 * 0.7
= 0.3429

Two are correct
  0.7 * 0.7 * 0.3
+ 0.7 * 0.3 * 0.7
+ 0.3 * 0.7 * 0.7
= 0.4409

Two are wrong
  0.3 * 0.3 * 0.7
+ 0.3 * 0.7 * 0.3
+ 0.7 * 0.3 * 0.3
= 0.189

All three are wrong
  0.3 * 0.3 * 0.3
= 0.027
</pre>
<p>We see that most of the times (~44%) the majority vote corrects an error. This majority vote ensemble will be correct an average of ~78% (0.3429 + 0.4409 = 0.7838).</p>
<h4>Number of voters</h4>
<p>Like repetition codes increase in their error-correcting capability when more codes are repeated, so do ensembles usually improve when adding more ensemble members.</p>
<p><a href="http://mlwave.com/wp-content/uploads/2015/06/Repetition_Code_On_Fading_Channel_Graph.jpg"><img class="aligncenter size-full wp-image-888" src="http://mlwave.com/wp-content/uploads/2015/06/Repetition_Code_On_Fading_Channel_Graph.jpg" alt="Repetition codes performance on graph" width="750" height="556" /></a></p>
<p>Using the same pinch of maths as above: a voting ensemble of 5 pseudo-random classifiers with 70% accuracy would be correct ~83% of the time. One or two errors are being corrected during ~66% of the majority votes. (0.36015 + 0.3087)</p>
<h4>Correlation</h4>
<p>When I first joined the team for KDD-cup 2014, Marios Michailidis (<a href="https://www.kaggle.com/kazanova">KazAnova</a>) proposed something peculiar. He calculated the <a href="http://onlinestatbook.com/2/describing_bivariate_data/pearson.html">Pearson correlation</a> for all our submission files and gathered a few well-performing models which were less correlated.</p>
<p>Creating an averaging ensemble from these diverse submissions gave us the biggest 50-spot jump on the leaderboard. Uncorrelated submissions clearly do better when ensembled than correlated submissions. But why?</p>
<p>To see this, let us take 3 simple models again. The ground truth is still all 1&#8242;s:</p>
<pre>1111111100 = 80% accuracy
1111111100 = 80% accuracy
1011111100 = 70% accuracy.
</pre>
<p>These models are highly correlated in their predictions. When we take a majority vote we see no improvement:</p>
<pre>1111111100 = 80% accuracy
</pre>
<p>Now we compare to 3 less-performing, but highly uncorrelated models:</p>
<pre>1111111100 = 80% accuracy
0111011101 = 70% accuracy
1000101111 = 60% accuracy
</pre>
<p>When we ensemble this with a majority vote we get:</p>
<pre>1111111101 = 90% accuracy
</pre>
<p>Which <i>is</i> an improvement: A lower correlation between ensemble model members seems to result in an increase in the error-correcting capability.</p>
<h4>Use for Kaggle: Forest Cover Type prediction</h4>
<p><img class="alignleft size-thumbnail wp-image-935" src="http://mlwave.com/wp-content/uploads/2015/06/forest2-150x100.png" alt="Forest" width="150" height="100" />Majority votes make most sense when the evaluation metric requires hard predictions, for instance with (multiclass-) classification accuracy.</p>
<p>The <a href="https://www.kaggle.com/c/forest-cover-type-prediction">forest cover type prediction</a> challenge uses the <a href="https://archive.ics.uci.edu/ml/datasets/Covertype">UCI Forest CoverType dataset</a>. The dataset has <span style="color: #123654;">54 attributes and there are 6 classes.</span></p>
<p>We create a simple <a href="https://www.kaggle.com/triskelion/forest-cover-type-prediction/first-try-with-random-forests">starter model</a> with a 500-tree Random Forest. We then create a few more models and pick the best performing one. For this task and our model selection an ExtraTreesClassifier works best.</p>
<h5>Weighing</h5>
<p>We then use a weighted majority vote. Why weighing? Usually we want to give a better model more weight in a vote. So in our case we count the vote by the best model 3 times. The other 4 models count for one vote each.</p>
<p>The reasoning is as follows: The only way for the inferior models to overrule the best model (expert) is for them to collectively (and confidently) agree on an alternative.</p>
<p>We can expect this ensemble to repair a few erroneous choices by the best model, leading to a small improvement only. That&#8217;s our punishment for forgoing a democracy and creating a Plato&#8217;s <i>Republic</i>.</p>
<blockquote><p>&#8220;Every city encompasses two cities that are at war with each other.&#8221; <cite>Plato in <i>The Republic</i></cite></p></blockquote>
<p>Table 1. shows the result of training 5 models, and the resulting score when combining these with a weighted majority vote.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Public Accuracy Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>GradientBoostingMachine</td>
<td>0.65057</td>
</tr>
<tr>
<td>RandomForest Gini</td>
<td>0.75107</td>
</tr>
<tr>
<td>RandomForest Entropy</td>
<td>0.75222</td>
</tr>
<tr>
<td>ExtraTrees Entropy</td>
<td>0.75524</td>
</tr>
<tr>
<td>ExtraTrees Gini (Best)</td>
<td><b>0.75571</b></td>
</tr>
<tr>
<td>Voting Ensemble (Democracy)</td>
<td>0.75337</td>
</tr>
<tr>
<td>Voting Ensemble (3*Best vs. Rest)</td>
<td><b>0.75667</b></td>
</tr>
</tbody>
</table>
<h4>Use for Kaggle: CIFAR-10 Object detection in images</h4>
<p><img class="alignright size-thumbnail wp-image-936" src="http://mlwave.com/wp-content/uploads/2015/06/cifar2-150x100.png" alt="CIFAR-10" width="150" height="100" />CIFAR-10 is another multi-class classification challenge where accuracy matters.</p>
<p>Our team leader for this challenge, <a href="https://www.kaggle.com/philculliton">Phil Culliton</a>, first found the best setup to <a href="http://blog.kaggle.com/2015/01/02/cifar-10-competition-winners-interviews-with-dr-ben-graham-phil-culliton-zygmunt-zajac/">replicate a good model</a> from dr. Graham.</p>
<p>Then he used a voting ensemble of around 30 convnets submissions (all scoring above 90% accuracy). The best single model of the ensemble scored <strong>0.93170</strong>.</p>
<p>A voting ensemble of 30 models scored <strong>0.94120</strong>. A ~0.01 reduction in error rate, pushing the resulting score beyond the <a href="http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/">estimated human classification accuracy</a>.</p>
<h4>Code</h4>
<p>We have a sample <a href="https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_vote.py">voting script</a> you could use at the MLWave Github repo. It operates on a directory of Kaggle submissions and creates a new submission.</p>
<blockquote><p>Ensembling. Train 10 neural networks and average their predictions. It’s a fairly trivial technique that results in easy, sizeable performance improvements.</p>
<p>One may be mystified as to why averaging helps so much, but there is a simple reason for the effectiveness of averaging. Suppose that two classifiers have an error rate of 70%. Then, when they agree they are right. But when they disagree, one of them is often right, so now the average prediction will place much more weight on the correct answer.</p>
<p>The effect will be especially strong whenever the network is confident when it’s right and unconfident when it’s wrong. <cite>Ilya Sutskever <a href="http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html">A brief overview of Deep Learning.</a></cite></p></blockquote>
<h3>Averaging</h3>
<p>Averaging works well for a wide range of problems (both classification and regression) and metrics (AUC, squared error or logaritmic loss).</p>
<p>There is not much more to averaging than taking the mean of individual model predictions. An often heard shorthand for this on Kaggle is &#8220;bagging submissions&#8221;.</p>
<p>Averaging predictions often reduces overfit. You ideally want a smooth separation between classes, and a single model&#8217;s predictions can be a little rough around the edges.</p>
<p><img class="aligncenter size-full wp-image-904" src="http://mlwave.com/wp-content/uploads/2015/06/overfit.png" alt="Learning from noise" width="531" height="248" /></p>
<p>The above image is from the Kaggle competition: <a href="https://www.kaggle.com/c/overfitting">Don&#8217;t Overfit!</a>, the black line shows a better separation than the green line. The green line has learned from noisy datapoints. No worries! Averaging multiple different green lines should bring us closer to the black line.</p>
<p>Remember our goal is not to memorize the training data (there are far more efficient ways to store data than inside a random forest), but to generalize well to new unseen data.</p>
<h4>Kaggle use: Bag of Words Meets Bags of Popcorn</h4>
<p><img class="alignleft size-thumbnail wp-image-933" src="http://mlwave.com/wp-content/uploads/2015/06/popcorn-150x150.png" alt="Icons" width="150" height="150" />This is a <a href="https://www.kaggle.com/c/word2vec-nlp-tutorial/">movie sentiment analysis contest</a>. In a previous post we used an <a href="http://mlwave.com/online-learning-perceptron/">online perceptron script</a> to get 95.2 AUC.</p>
<p>The perceptron is a decent linear classifier which is guaranteed to find a separation if the data is linearly separable. This is a welcome property to have, but you have to realize a perceptron stops learning once this separation is reached. It does not necessarily find the best separation for new data.</p>
<p>So what would happen if we initialize 5 perceptrons with random weights and combine their predictions through an average? Why, we get an improvement on the test set!</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Public AUC Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>Perceptron</td>
<td>0.95288</td>
</tr>
<tr>
<td>Random Perceptron</td>
<td>0.95092</td>
</tr>
<tr>
<td>Random Perceptron</td>
<td>0.95128</td>
</tr>
<tr>
<td>Random Perceptron</td>
<td>0.95118</td>
</tr>
<tr>
<td>Random Perceptron</td>
<td>0.95072</td>
</tr>
<tr>
<td>Bagged Perceptrons</td>
<td><b>0.95427</b></td>
</tr>
</tbody>
</table>
<p>Above results also illustrate that ensembling can (temporarily) save you from having to learn about the finer details and inner workings of a specific Machine Learning algorithm. If it works, great! If it doesn&#8217;t, not much harm done.</p>
<p><img class="aligncenter size-full wp-image-909" src="http://mlwave.com/wp-content/uploads/2015/06/perceptron-bagging.png" alt="Perceptron bagging" width="550" height="329" /></p>
<p>You also won&#8217;t get a penalty for averaging 10 exactly the same linear regressions. Bagging a single poorly cross-validated and overfitted submission may even bring you some gain through adding diversity (thus less correlation).</p>
<h4>Code</h4>
<p>We have posted a simple <a href="https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_avg.py">averaging script</a> on Github that takes as input a directory of .csv files and outputs an averaged submission. Update: Dat Le has added a <a href="https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_geomean.py">geometric averaging script</a>. Geometric mean can outperform a plain average.</p>
<h3>Rank averaging</h3>
<p>When averaging the outputs from multiple different models some problems can pop up. Not all predictors are perfectly <a href="https://www.kaggle.com/cbourguignat/otto-group-product-classification-challenge/why-calibration-works">calibrated</a>: they may be over- or underconfident when predicting a low or high probability. Or the predictions clutter around a certain range.</p>
<p>In the extreme case you may have a submission which looks like this:</p>
<pre>Id,Prediction
1,0.35000056
2,0.35000002
3,0.35000098
4,0.35000111
</pre>
<p>Such a prediction may do well on the leaderboard when the evaluation metric is ranking or threshold based like AUC. But when averaged with another model like:</p>
<pre>Id,Prediction
1,0.57
2,0.04
3,0.96
4,0.99
</pre>
<p>it will not change the ensemble much at all.</p>
<p>Our solution is to first turn the predictions into ranks, then averaging these ranks.</p>
<pre>Id,Rank,Prediction
1,1,0.35000056
2,0,0.35000002
3,2,0.35000098
4,3,0.35000111
</pre>
<p>After normalizing the averaged ranks between 0 and 1 you are sure to get an even distribution in your predictions. The resulting rank-averaged ensemble:</p>
<pre>Id,Prediction
1,0.33
2,0.0
3,0.66
4,1.0
</pre>
<h4>Historical ranks.</h4>
<p>Ranking requires a test set. So what do you do when want predictions for a single new sample? You could rank it together with the old test set, but this will increase the complexity of your solution.</p>
<p>A solution is using historical ranks. Store the old test set predictions together with their rank. Now when you predict a new test sample like &#8220;0.35000110&#8243; you find the closest old prediction and take its historical rank (in this case rank &#8220;3&#8243; for &#8220;0.35000111&#8243;).</p>
<h4>Kaggle use case: Acquire Valued Shoppers Challenge</h4>
<p><img class="alignright size-thumbnail wp-image-938" src="http://mlwave.com/wp-content/uploads/2015/06/shoppers-150x100.png" alt="Scissors" width="150" height="100" />Ranking averages do well on ranking and threshold-based metrics (like AUC) and search-engine quality metrics (like average precision at k).</p>
<p>The goal of the <a href="https://www.kaggle.com/c/acquire-valued-shoppers-challenge">shopper challenge</a> was to rank the chance that a shopper would become a repeat customer.</p>
<p>Our team first took an average of multiple Vowpal Wabbit models together with an R GLMNet model. Then we used a ranking average to improve the exact same ensemble.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Public</th>
<th>Private</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vowpal Wabbit A</td>
<td>0.60764</td>
<td>0.59962</td>
</tr>
<tr>
<td>Vowpal Wabbit B</td>
<td>0.60737</td>
<td>0.59957</td>
</tr>
<tr>
<td>Vowpal Wabbit C</td>
<td>0.60757</td>
<td>0.59954</td>
</tr>
<tr>
<td>GLMNet</td>
<td>0.60433</td>
<td>0.59665</td>
</tr>
<tr>
<td>Average Bag</td>
<td>0.60795</td>
<td>0.60031</td>
</tr>
<tr>
<td>Rank average Bag</td>
<td>0.61027</td>
<td><strong>0.60187</strong></td>
</tr>
</tbody>
</table>
<p>I already wrote about the <a href="http://mlwave.com/lessons-from-avito-prohibited-content-kaggle/">Avito challenge</a> where rank averaging gave us a hefty increase.</p>
<p>Finally, when weighted rank averaging the bagged perceptrons from the previous chapter (1x) with the new <a href="http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/">bag-of-words tutorial</a> (3x) on fastML.com we improve that model&#8217;s performance from 0.96328 AUC to <span style="color: #444444;">0.96461 AUC.</span></p>
<h4>Code</h4>
<p>A simple work-horse <a href="https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_rankavg.py">rank averaging script</a> is added to the MLWave Github repo.</p>
<blockquote><p>Competitions are effective because there are any number of techniques that can be applied to any modeling problem, but we can&#8217;t know in advance which will be most effective. <cite>Anthony Goldbloom <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=5693459&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F5691154%2F5693274%2F05693459.pdf%3Farnumber%3D5693459">Data Prediction Competitions &#8212; Far More than Just a Bit of Fun</a></cite></p></blockquote>
<p><img class="aligncenter size-full wp-image-961" src="http://mlwave.com/wp-content/uploads/2015/06/blending.jpg" alt="Whiskey blending" width="500" height="281" /></p>
<p><small>From &#8216;How Scotch Blended Whisky is Made&#8217; on <a href="https://www.youtube.com/watch?v=8vCZVsy0jIY">Youtube</a></small></p>
<h2>Stacked Generalization &amp; Blending</h2>
<p>Averaging prediction files is nice and easy, but it&#8217;s not the only method that the <a href="https://www.kaggle.com/users">top Kagglers</a> are using. The serious gains start with stacking and blending. Hold on to your top-hats and petticoats: Here be dragons. With 7 heads. Standing on top of 30 other dragons.</p>
<h3>Netflix</h3>
<p>Netflix organized and popularized the first data science competitions. Competitors in the movie recommendation challenge really pushed the state of the art on ensemble creation, perhaps so much so that Netflix decided <a href="http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html">not to implement</a> the winning solution in production. That one was simply too complex.</p>
<p>Nevertheless, a number of papers and novel methods resulted from this challenge:</p>
<ul>
<li><a href="http://arxiv.org/pdf/0911.0460.pdf">Feature-Weighted Linear Stacking</a></li>
<li><a href="http://elf-project.sourceforge.net/CombiningPredictionsForAccurateRecommenderSystems.pdf">Combining Predictions for Accurate Recommender Systems</a></li>
<li><a href="http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf">The BigChaos Solution to the Netflix Prize</a></li>
</ul>
<p>All are interesting, accessible and relevant reads when you want to improve your Kaggle game.</p>
<p><a href="http://mlwave.com/wp-content/uploads/2015/06/NetflixPrize.png"><img class="aligncenter size-full wp-image-956" src="http://mlwave.com/wp-content/uploads/2015/06/NetflixPrize.png" alt="Netflix Prize Leaderboard" width="771" height="482" /></a></p>
<blockquote><p>This is a truly impressive compilation and culmination of years of work, blending hundreds of predictive models to finally cross the finish line. We evaluated some of the new methods offline but the additional accuracy gains that we measured did not seem to justify the engineering effort needed to bring them into a production environment. <cite>Netflix Engineers</cite></p></blockquote>
<h3>Stacked generalization</h3>
<p>Stacked generalization was introduced by Wolpert in a <a href="http://www.researchgate.net/profile/David_Wolpert/publication/222467943_Stacked_generalization/links/0c960529e2b49a95f2000000.pdf">1992 paper</a>, 2 years before the seminal Breiman paper &#8220;<a href="http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf">Bagging Predictors</a>&#8220;. Wolpert is famous for another very popular machine learning theorem: &#8220;<a href="http://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization">There is no free lunch in search and optimization</a>&#8220;.</p>
<p>The basic idea behind stacked generalization is to use a pool of base classifiers, then using another classifier to combine their predictions, with the aim of reducing the generalization error.</p>
<p>Let&#8217;s say you want to do 2-fold stacking:</p>
<ul>
<li>Split the train set in 2 parts: train_a and train_b</li>
<li>Fit a first-stage model on train_a and create predictions for train_b</li>
<li>Fit the same model on train_b and create predictions for train_a</li>
<li>Finally fit the model on the entire train set and create predictions for the test set.</li>
<li>Now train a second-stage stacker model on the probabilities from the first-stage model(s).</li>
</ul>
<p>A stacker model gets more information on the problem space by using the first-stage predictions as features, than if it was trained in isolation.</p>
<blockquote><p>It is usually desirable that the level 0 generalizers are of all &#8220;types&#8221;, and not just simple variations of one another (e.g., we want surface-fitters, Turing-machine builders, statistical extrapolators, etc., etc.). In this way all possible ways of examining the learning set and trying to extrapolate from it are being exploited. This is part of what is meant by saying that the level 0 generalizers should &#8220;span the space&#8221;.</p>
<p>[...] stacked generalization is a means of non-linearly combining generalizers to make a new generalizer, to try to optimally integrate what each of the original generalizers has to say about the learning set. The more each generalizer has to say (which isn’t duplicated in what the other generalizer’s have to say), the better the resultant stacked generalization. <cite>Wolpert (1992) Stacked Generalization</cite></p></blockquote>
<h3>Blending</h3>
<p>Blending is a word introduced by the Netflix winners. It is very close to stacked generalization, but a bit simpler and less risk of an information leak. Some researchers use &#8220;stacked ensembling&#8221; and &#8220;blending&#8221; interchangeably.</p>
<p>With blending, instead of creating out-of-fold predictions for the train set, you create a small holdout set of say 10% of the train set. The stacker model then trains on this holdout set only.</p>
<p>Blending has a few benefits:</p>
<ul>
<li>It is simpler than stacking.</li>
<li>It wards against an information leak: The generalizers and stackers use different data.</li>
<li>You do not need to share a seed for stratified folds with your teammates. Anyone can throw models in the &#8216;blender&#8217; and the blender decides if it wants to keep that model or not.</li>
</ul>
<p>The cons are:</p>
<ul>
<li>You use less data overall</li>
<li>The final model may overfit to the holdout set.</li>
<li>Your CV is more solid with stacking (calculated over more folds) than using a single small holdout set.</li>
</ul>
<p>As for performance, both techniques are able to give similar results, and it seems to be a matter of preference and skill which you prefer. I myself prefer stacking.</p>
<p>If you can not choose, you can always do both. Create stacked ensembles with stacked generalization and out-of-fold predictions. Then use a holdout set to further combine these models at a third stage.</p>
<h3>Stacking with logistic regression</h3>
<p>Stacking with logistic regression is one of the more basic and traditional ways of stacking. A <a href="https://github.com/emanuele/kaggle_pbr/blob/master/blend.py">script</a> I found by <a href="https://www.kaggle.com/emanuele">Emanuele Olivetti</a> helped me understand this.</p>
<p>When creating predictions for the test set, you can do that in one go, or take an average of the out-of-fold predictors. Though taking the average is the clean and more accurate way to do this, I still prefer to do it in one go as that slightly lowers both model and coding complexity.</p>
<h4>Kaggle use: &#8220;Papirusy z Edhellond&#8221;</h4>
<p><img class="alignleft size-thumbnail wp-image-995" src="http://mlwave.com/wp-content/uploads/2015/06/spam-competition-150x100.png" alt="Gondor" width="150" height="100" />I used the above blend.py script by Emanuele to compete in this inClass competition. Stacking 8 base models (diverse ET&#8217;s, RF&#8217;s and GBM&#8217;s) with Logistic Regression gave me my second best score of 0.99409 accuracy, good for first place.</p>
<h4>Kaggle use: KDD-cup 2014</h4>
<p>Using this script I was able to improve a model from <a href="https://www.kaggle.com/yansoftware">Yan Xu</a>. Her model before stacking scored ~0.605 AUC. With stacking this improved to ~0.625.</p>
<h3>Stacking with non-linear algorithms</h3>
<p>Popular non-linear algorithms for stacking are GBM, KNN, NN, RF and ET.</p>
<p>Non-linear stacking with the original features on multiclass problems gives surprising gains. Obviously the first-stage predictions are very informative and get the highest feature importance. Non-linear algorithms find useful interactions between the original features and the meta-model features.</p>
<h4>Kaggle use: TUT Headpose Estimation Challenge</h4>
<p><img class="alignright size-thumbnail wp-image-997" src="http://mlwave.com/wp-content/uploads/2015/06/tut-headpose2-150x100.png" alt="TUT headpose" width="150" height="100" /> The <a href="https://inclass.kaggle.com/c/tut-head-pose-estimation-challenge">TUT Headpose Estimation</a> challenge can be treated as a multi-class multi-label classification challenge.</p>
<p>For every label a separate ensemble model was trained.</p>
<p>The following table shows the result of training individual models, and their improved scores when stacking the predicted class probabilities with an extremely randomized trees model.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Public MAE</th>
<th>Private MAE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Forests 500 estimators</td>
<td>6.156</td>
<td>6.546</td>
</tr>
<tr>
<td>Extremely Randomized Trees 500 estimators</td>
<td>6.317</td>
<td>6.666</td>
</tr>
<tr>
<td>KNN-Classifier with 5 neighbors</td>
<td>6.828</td>
<td>7.460</td>
</tr>
<tr>
<td>Logistic Regression</td>
<td>6.694</td>
<td>6.949</td>
</tr>
<tr>
<td>Stacking with Extremely Randomized Trees</td>
<td><b>4.772</b></td>
<td><b>4.718</b></td>
</tr>
</tbody>
</table>
<p>We see that stacked generalization with standard models is able to reduce the error by around 30%(!).</p>
<p>Read more about this result in the paper: <a href="http://vision.cs.tut.fi/data/publications/scia2015_hpe.pdf">Computer Vision for Head Pose Estimation: Review of a Competition</a>.</p>
<h4>Code</h4>
<p>You can find a function to create <a href="https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/blend_proba.py">out-of-fold probability predictions</a> in the MLWave Github repo. You could use numpy horizontal stacking (hstack) to create blended datasets.</p>
<h3>Feature weighted linear stacking</h3>
<p>Feature-weighted linear stacking stacks engineered meta-features together with model predictions. The hope is that the stacking model learns which base model is the best predictor for samples with a certain feature value. Linear algorithms are used to keep the resulting model fast and simple to inspect.</p>
<p><img class="aligncenter size-full wp-image-948" src="http://mlwave.com/wp-content/uploads/2015/06/feature-weighted-stacking.png" alt="Blended prediction" width="492" height="93" /></p>
<p>Vowpal Wabbit can implement a form of feature-weighted linear stacking out of the box. If we have a train set like:</p>
<pre><small>1 |f f_1:0.55 f_2:0.78 f_3:7.9 |s RF:0.95 ET:0.97 GBM:0.92</small>
</pre>
<p>We can add quadratic feature interactions between the <code>s</code>-featurespace and the <code>f</code>-featurespace by adding <code>-q fs</code>. The features in the <code>f</code>-namespace can be engineered meta-features like in the paper, or they can be the original features.</p>
<h3>Quadratic linear stacking of models</h3>
<p>This did not have a name so I made one up. It is very similar to feature-weighted linear stacking, but it creates combinations of model predictions. This improved the score on numerous experiments, most noticeably on the <a href="http://www.drivendata.org/competitions/6/">Modeling Women&#8217;s Healthcare Decision competition</a> on DrivenData.</p>
<p>Using the same VW training set as before:</p>
<pre><small>1 |f f_1:0.55 f_2:0.78 f_3:7.9 |s RF:0.95 ET:0.97 GBM:0.92</small>
</pre>
<p>We can train with <code>-q ss</code> creating quadratic feature interactions (<code>RF*GBM</code>) between the model predictions.</p>
<p>This can easily be combined with feature-weighted linear stacking: <code>-q fs -q ss</code>, possibly improving on both.</p>
<blockquote><p>So now you have a case where many base models should be created. You don&#8217;t know apriori which of these models are going to be helpful in the final meta model. In the case of two stage models, it is highly likely weak base models are preferred.</p>
<p>So why tune these base models very much at all? Perhaps tuning here is just obtaining model diversity. But at the end of the day you don&#8217;t know which base models will be helpful. And the final stage will likely be linear (which requires no tuning, or perhaps a single parameter to give some sparsity).  <cite><a href="https://www.kaggle.com/mikeskim">Mike Kim</a> <a href="https://www.kaggle.com/forums/f/15/kaggle-forum/t/14469/tuning-doesn-t-matter-why-are-you-doing-it/">Tuning doesn&#8217;t matter. Why are you doing it?</a></cite></p></blockquote>
<h3>Stacking classifiers with regressors and vice versa</h3>
<p>Stacking allows you to use classifiers for regression problems and vice versa. For instance, one may try a base model with quantile regression on a binary classification problem. A good stacker should be able to take information from the predictions, even though usually regression is not the best classifier.</p>
<p>Using classifiers for regression problems is a bit trickier. You use binning first: You turn the y-label into evenly spaced classes. A regression problem that requires you to predict wages can be turned into a multiclass classification problem like so:</p>
<ul>
<li>Everything under 20k is class 1.</li>
<li>Everything between 20k and 40k is class 2.</li>
<li>Everything over 40k is class 3.</li>
</ul>
<p>The predicted probabilities for these classes can help a stacking regressor make better predictions.</p>
<blockquote><p>&#8220;I learned that you never, ever, EVER go anywhere without your out-of-fold predictions. If I go to Hawaii or to the bathroom I am bringing them with. Never know when I need to train a 2nd or 3rd level meta-classifier&#8221; <cite><a href="https://www.kaggle.com/scharf">T. Sharf</a></cite></p></blockquote>
<h3>Stacking unsupervised learned features</h3>
<p>There is no reason we are restricted to using supervised learning techniques with stacking. You can also stack with unsupervised learning techniques.</p>
<p>K-Means clustering is a popular technique that makes sense here. Sofia-ML implements a fast online k-means algorithm suitable for this.</p>
<p>Another more recent interesting addition is to use <a href="http://lvdmaaten.github.io/tsne/">t-SNE</a>: Reduce the dataset to 2 or 3 dimensions and stack this with a non-linear stacker. Using a holdout set for stacking/blending feels like the safest choice here. See here for a solution by <a href="https://www.kaggle.com/mikeskim">Mike Kim</a>, using t-SNE vectors and boosting them with XGBoost: &#8216;<a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14295/41599-via-tsne-meta-bagging">0.41599 via t-SNE meta-bagging</a>&#8216;.</p>
<p><a href="http://mlwave.com/wp-content/uploads/2015/06/t-sne.png"><img class="aligncenter size-full wp-image-1006" src="http://mlwave.com/wp-content/uploads/2015/06/t-sne.png" alt="t-SNE" width="550" height="516" /></a></p>
<p><small><a href="https://www.kaggle.com/piotrw">Piotr</a> shows a nice visualization with t-SNE on the Otto Product Classification Challenge data set.</small></p>
<h3>Online Stacking</h3>
<p>I spend quit a lot of time working out an idea I had for online stacking: first create small fully random trees from the hashed binary representation. Substract profit or add profit when the tree makes a correct prediction. Now take the most profitable and least profitable trees and add them to the feature representation.</p>
<p>It worked, but only on artificial data. For instance, a linear perceptron with online random tree stacking was able to learn a non-linear XOR-problem. It did not work on any real-life data I tried it on, and believe me, I tried. So from now on I&#8217;ll be suspicious of papers which only feature artificial data sets to showcase their new algorithm.</p>
<p>A similar idea did work for the author of the paper: <a href="http://arxiv.org/abs/1501.02990">random bit regression.</a> Here many random linear functions are created from the features, and the best are found through heavy regularization. This I was able to replicate with success on some datasets. This will the topic of a future post.</p>
<p>A more concrete example of (semi-) online stacking is with ad click prediction. Models trained on recent data perform better there. So when a dataset has a temporal effect, you could use Vowpal Wabbit to train on the entire dataset, and use a more complex and powerful tool like XGBoost to train on the last day of data. Then you stack the XGBoost predictions together with the samples and let Vowpal Wabbit do what it does best: optimizing loss functions.</p>
<blockquote><p>The natural world is complex, so it figures that ensembling different models can capture more of this complexity. <cite>Ben Hamner &#8216;Machine learning best practices we&#8217;ve learned from hundreds of competitions&#8217; (<a href="https://www.youtube.com/watch?v=9Zag7uhjdYo">video</a>)</cite></p></blockquote>
<h3>Everything is a hyper-parameter</h3>
<p>When doing stacking/blending/meta-modeling it is healthy to think of every action as a hyper-parameter for the stacker model.</p>
<p>So for instance:</p>
<ul>
<li>Not scaling the data</li>
<li>Standard-Scaling the data</li>
<li>Minmax scaling the data</li>
</ul>
<p>are simply extra parameters to be tuned to improve the ensemble performance. Likewise, the number of base models to use can be seen as a parameter to optimize. Feature selection (top 70%) or imputation (impute missing features with a 0) are other examples of meta-parameters.</p>
<p>Like a random gridsearch is a good candidate for tuning algorithm parameters, so does it work for tuning these meta-parameters.</p>
<blockquote><p>Sometimes it is useful to allow XGBoost to see what a KNN-classifier sees. &#8211; <cite><a href="http://blog.kaggle.com/2015/05/07/profiling-top-kagglers-kazanovacurrently-2-in-the-world/">Marios Michailidis</a></cite></p></blockquote>
<h3>Model Selection</h3>
<p>You can further optimize scores by combining multiple ensembled models.</p>
<ul>
<li>There is the ad-hoc approach: Use averaging, voting or rank averaging on manually-selected well-performing ensembles.</li>
<li>Greedy forward model selection (<a href="http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf">Caruana et al.</a>). Start with a base ensemble of 3 or so good models. Add a model when it increases the train set score the most. By allowing put-back of models, a single model may be picked multiple times (weighing).</li>
<li>Genetic model selection uses genetic algorithms and CV-scores as the fitness function. See for instance <a href="https://www.kaggle.com/inversion">inversion</a>&#8216;s solution &#8216;<a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14315/strategy-for-top-25-score">Strategy for top 25 position</a>&#8216;.</li>
<li>I use a fully random method inspired by Caruana&#8217;s method: Create a 100 or so ensembles from randomly selected ensembles (without placeback). Then pick the highest scoring model.</li>
</ul>
<h3>Automation</h3>
<p><img class="alignleft size-thumbnail wp-image-1026" src="http://mlwave.com/wp-content/uploads/2015/06/thumb76_76-150x150.png" alt="Otto Group" width="150" height="150" />When stacking for the <a href="https://www.kaggle.com/c/otto-group-product-classification-challenge">Otto product classification</a> competition I quickly got a good top 10 spot. Adding more and more base models and bagging multiple stacked ensembles I was able to keep improving my score.</p>
<p>Once I had reached 7 base models stacked by 6 stackers, a sense of panic and gloom started to set in. Would I be able to replicate all of this? These complex and slow unwieldy models were out of my comfort zone of fast and simple Machine Learning.</p>
<p>I spend the rest of the competition building a way to automate stacking. For base models pure random algorithms with pure random parameters are trained. Wrappers were written to make classifiers like VW, Sofia-ML, RGF, MLP and XGBoost play nicely with the Scikit-learn API.</p>
<p><img class="aligncenter size-full wp-image-1028" src="http://mlwave.com/wp-content/uploads/2015/06/whiteboard-automated-stacking.png" alt="Whiteboard automated stacking" width="550" height="279" /><br />
<small><i>The first whiteboard sketch for a parallelized automated stacker with 3 buckets</i></small></p>
<p>For stackers I let the script use SVM, random forests, extremely randomized trees, GBM and XGBoost with random parameters and a random subset of base models.</p>
<p>Finally the created stackers are averaged when their fold-predictions on the train set produced a lower loss.</p>
<p>This automated stacker was able to rank 57th spot a week before the competition ended. It contributed to my final ensemble. The only difference was I never spend time tuning or selecting: I started the script, went to bed, and awoke to a good solution.</p>
<p><a href="http://mlwave.com/wp-content/uploads/2015/06/otto.png"><img class="aligncenter size-full wp-image-973" src="http://mlwave.com/wp-content/uploads/2015/06/otto.png" alt="Otto Leaderboard" width="600" height="102" /></a></p>
<p><small><i>The automated stacker is able to get a top 10% score without any tuning or manual model selection on a competitive task with over 3000 competitors.</i></small></p>
<p>Automatic stacking is one of my new big interests. Expect a few follow-up articles on this. The best result of automatic stacking was found on the TUT Headpose Estimation challenge. This black-box solution beats the current state-of-the-art set by domain experts who created special-purpose algorithms for this particular problem.</p>
<p><a href="http://mlwave.com/wp-content/uploads/2015/06/tut-headpose.png"><img class="aligncenter size-full wp-image-970" src="http://mlwave.com/wp-content/uploads/2015/06/tut-headpose.png" alt="Tut headpose leaderboard" width="600" height="260" /></a></p>
<p>Noteworthy: This was a multi-label classification problem. Predictions for both &#8220;yaw&#8221; and &#8220;pitch&#8221; were required. Since the &#8220;yaw&#8221; and &#8220;pitch&#8221;-labels of a head pose are interrelated, stacking a model with predictions for &#8220;yaw&#8221; increased the accuracy for &#8220;pitch&#8221; predictions and vice versa. An interesting result.</p>
<p>Models visualized as a network can be trained used back-propagation: then stacker models learn which base models reduce the error the most.</p>
<p><img class="aligncenter size-full wp-image-989" src="http://mlwave.com/wp-content/uploads/2015/06/otto-backprop.png" alt="Ensemble Network" width="550" height="361" /></p>
<p>Next to CV-scores one could take the standard deviation of the CV-scores into account (a smaller deviation is a safer choice). One could look at optimizing complexity/memory usage and running times. Finally one can look at adding correlation into the mix &#8212; make the script prefer uncorrelated model predictions when creating ensembles.</p>
<p>The entire automated stacking pipeline can be parallelized and distributed. This also brings speed improvements and faster good results on a single laptop.</p>
<p>Contextual bandit optimization seems like a good alternative to fully random gridsearch: We want our algorithm to start exploiting good parameters and models and remember that the random SVM it picked last time ran out of memory. These additions to stacking will be explored in greater detail soon.</p>
<p>In the meantime you can get a sneak preview on the MLWave Github repo: &#8220;<a href="https://github.com/MLWave/hodor-autoML">Hodor-autoML</a>&#8220;.</p>
<p>The #1 and #2 winners of the Otto product classification challenge used ensembles of over a 1000 different models. Read more about the <a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov">first place</a> and the <a href="http://blog.kaggle.com/2015/06/09/otto-product-classification-winners-interview-2nd-place-alexander-guschin/">second place</a>.</p>
<h2>Why create these Frankenstein ensembles?</h2>
<p>You may wonder why this exercise in futility: stacking and combining 1000s of models and computational hours is insanity right? Well&#8230; yes. But these monster ensembles still have their uses:</p>
<ul>
<li>You can win Kaggle competitions.</li>
<li>You can beat most state-of-the-art academic benchmarks with a single approach.</li>
<li>You can then compare your new-and-improved benchmark with the performance of a simpler, more production-friendly model</li>
<li>One day, today&#8217;s computers and clouds will seem weak. You&#8217;ll be ready.</li>
<li>It is possible to transfer knowledge from the ensemble back to a simpler shallow model (Hinton&#8217;s <a href="http://www.ttic.edu/dl/dark14.pdf">Dark Knowledge</a>, Caruana&#8217;s <a href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf">Model Compression</a>)</li>
<li>Not all base models necessarily need to finish in time. In that regard, ensembling introduces a form of graceful degradation: loss of one model is not fatal for creating good predictions.</li>
<li>Automated large ensembles ward against overfit and add a form of regularization, without requiring much tuning or selection. In principle stacking could be used by lay-people.</li>
<li>It is currently one of the best methods to improve machine learning algorithms, perhaps telling use something about efficient <a href="http://mlwave.com/human-ensemble-learning/">human ensemble learning</a>.</li>
<li>A 1% increase in accuracy may push an investment fund from making a loss, into making a little less loss. More seriously: Improving healthcare screening methods helps save lives.</li>
</ul>
<p>Update: Thanks a lot to <a href="https://github.com/lenguyenthedat">Dat Le</a> for documenting and refactoring the <a href="https://github.com/MLWave/Kaggle-Ensemble-Guide">code</a> accompanying this article. Thanks a lot everyone for the encouraging comments. My apologies if I have forgotten to link to your previous inspirational work. Further reading at &#8220;<a href="http://www.overkillanalytics.net/more-is-always-better-the-power-of-simple-ensembles/">More is always better &#8211; The power of Simple Ensembles</a>&#8221; by <a href="https://www.kaggle.com/cartersibley">Carter Sibley</a>, &#8220;<a href="https://www.kaggle.com/c/tradeshift-text-classification/forums/t/10629/benchmark-with-sklearn/">Tradeshift Benchmark Tutorial with two-stage SKLearn models</a>&#8221; by <a href="https://www.kaggle.com/dremovd">Dmitry Dryomov</a>, &#8220;<a href="http://www.chioka.in/stacking-blending-and-stacked-generalization/">Stacking, Blending and Stacked Generalization</a>&#8221; by <a href="https://www.kaggle.com/ericchio">Eric Chio</a>, <a href="http://www.slideshare.net/liorrokach/ensemble-learning-the-wisdom-of-crowds-of-machines">Ensemble Learning: The wisdom of the crowds (of machines)</a> by <a href="http://www.ise.bgu.ac.il/faculty/liorr/">Lior Rokach</a>, and &#8220;<a href="http://videolectures.net/roks2013_wiering_vector/">Deep Support Vector Machines</a>&#8221; by <a href="http://www.ai.rug.nl/~mwiering/">Marco Wiering</a>.</p>
<p><small>Terminology: When I say ensembling I mean &#8216;model averaging&#8217;: combining multiple models. Algorithms like Random Forests use ensembling techniques like bagging internally. For this article we are not interested in that.</small></p>
<p><small>The intro image came from WikiMedia Commons and is in the public domain, courtesy of <a href="https://en.wikipedia.org/wiki/User:Merzperson">Jesse Merz</a>.</small></p>
]]></content:encoded>
			<wfw:commentRss>http://mlwave.com/kaggle-ensembling-guide/feed/</wfw:commentRss>
		<slash:comments>29</slash:comments>
		</item>
		<item>
		<title>Online Learning Perceptron</title>
		<link>http://mlwave.com/online-learning-perceptron/</link>
		<comments>http://mlwave.com/online-learning-perceptron/#comments</comments>
		<pubDate>Sat, 24 Jan 2015 22:00:35 +0000</pubDate>
		<dc:creator><![CDATA[mladmin]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://mlwave.com/?p=808</guid>
		<description><![CDATA[Let&#8217;s take a look at the perceptron: the simplest artificial neuron. This article goes from a concept devised in 1943 to a Kaggle competition in 2015. It shows that a single artificial neuron can get 0.95 AUC on an NLP sentiment analysis task (predicting if a movie review is positive or negative). In logic there are [&#8230;]]]></description>
				<content:encoded><![CDATA[<p><strong>Let&#8217;s take a look at the perceptron: the simplest artificial neuron. This article goes from a concept devised in 1943 to a Kaggle competition in 2015. It shows that a single artificial neuron can get 0.95 AUC on an NLP sentiment analysis task (predicting if a movie review is positive or negative).</strong></p>
<p><em>In logic there are no morals. Everyone is at liberty to build up his own logic, i.e., his own form of language, as he wishes.</em> &#8211; Rudolf Carnap (1934) &#8220;Logical Syntax of Language&#8221;</p>
<h2>McCulloch-Pitts Neuron</h2>
<p>The birth of artificial neural nets started with the 1943 paper <a href="http://deeplearning.cs.cmu.edu/pdfs/McCulloch.and.Pitts.pdf">&#8220;a<br />
Logical Calculus of the Ideas Immanent in Nervous Activity&#8221;</a>. Two researchers, McCulloch a neurologist, Pitts a logician, joined forces to sketch out the first artificial neurons.</p>
<p><span id="more-808"></span></p>
<p>McCulloch reasoned that nervous activity had an &#8216;all-or-nothing&#8217; activity: A neuron would fire (output &#8220;1&#8243;) once it&#8217;s activation threshold was reached, or it wouldn&#8217;t fire (output &#8220;0&#8243;).<br />
Pitts saw and understood the potential to capture propositional logic using such neurological principles.</p>
<div id="attachment_826" style="width: 651px" class="wp-caption aligncenter"><a href="http://mlwave.com/wp-content/uploads/2015/01/mccullogpitts.png"><img class="size-full wp-image-826" src="http://mlwave.com/wp-content/uploads/2015/01/mccullogpitts.png" alt="mccullog pitts neuron" width="641" height="126" /></a><p class="wp-caption-text">Schematics of neurons, looping neurons and nets from the McCulloch-Pitts paper.</p></div>
<h3>An example</h3>
<p>A bird will try to eat brown round objects, such as seeds. We can represent this behaviour as:</p>
<table>
<thead>
<tr>
<th>Object</th>
<th>Brown?</th>
<th>Round?</th>
<th>Eat?</th>
</tr>
</thead>
<tbody>
<tr>
<td>Seed</td>
<td>1</td>
<td>1</td>
<td><b>1</b></td>
</tr>
<tr>
<td>Leaf</td>
<td>1</td>
<td>0</td>
<td><b>0</b></td>
</tr>
<tr>
<td>Golf Ball</td>
<td>0</td>
<td>1</td>
<td><b>0</b></td>
</tr>
<tr>
<td>Key</td>
<td>0</td>
<td>0</td>
<td><b>0</b></td>
</tr>
</tbody>
</table>
<p>One could model the above table with a McCulloch-Pitts Neuron. If you set the activation threshold to 1.5, the neuron would only fire when both &#8220;brown&#8221; AND &#8220;round&#8221; properties are satisfied. Then the sum of the inputs is 2 (1+1), which is larger than the activation threshold of 1.5.</p>
<p><a href="http://mlwave.com/wp-content/uploads/2015/01/pitts-neuron.png"><img class="aligncenter size-full wp-image-820" src="http://mlwave.com/wp-content/uploads/2015/01/pitts-neuron.png" alt="Pitts Neuron" width="619" height="111" /></a></p>
<p><em>As a condition for the use of logical inferences and the performance of logical operations, something must already be given to our faculty of representation, certain extralogical concrete objects that are intuitively present as immediate experience prior to all thought.</em> &#8211; Hilbert (1926)</p>
<h2>Perceptron</h2>
<p>Invented in 1957 by cognitive psychologist Frank Rosenblatt, the perceptron algorithm was the first artificial neural net implemented in hardware.</p>
<p>In 1960 researchers at Cornell Aeronautical Laboratory, with funding from the US Office of Naval Research, randomly hooked 400 photocells to a perceptron and the &#8220;Mark 1 perceptron&#8221; was born. It was capable of basic image recognition.</p>
<h3>Weights</h3>
<p>A Perceptron works by assigning weights to incoming connections. With the McCulloch-Pitts Neuron we took the sum of the values from the incoming connections, then looked if it was over or below a certain threshold. With the Perceptron we instead take the dotproduct. We multiply each incoming value with a weight and take the sum: (value1 * weight1) + (value2 * weight2) etc.</p>
<p><a href="http://mlwave.com/wp-content/uploads/2015/01/perceptron-mark-1.png"><img class="aligncenter size-full wp-image-839" src="http://mlwave.com/wp-content/uploads/2015/01/perceptron-mark-1.png" alt="Perceptron mark 1" width="723" height="326" /></a></p>
<h3>Learning</h3>
<p>A perceptron is a supervised classifier. It learn by first making a prediction: Is the dotproduct over or below the threshold? If it over the threshold it predicts a &#8220;1&#8243;, if it is below threshold it predicts a &#8220;0&#8243;.</p>
<p>Then the perceptron looks at the label of the sample. If the prediction was correct, then the error is &#8220;0&#8243;, and it leaves the weights alone. If the prediction was wrong, the error is either &#8220;-1&#8243; or &#8220;1&#8243; and the perceptron will update the weights like:</p>
<pre><small>weights[feature_index] += learning_rate * error * feature_value</small></pre>
<p><i><small>Above code corrected: Thank you leakymirror.</small></i></p>
<h3>Example</h3>
<p>Here a perceptron learns to model a NAND function without any errors:</p>
<pre><small>"""
Creative Commons A-SA, source: http://en.wikipedia.org/wiki/Perceptron
"""
threshold = 0.5
learning_rate = 0.1
weights = [0, 0, 0]
training_set = [((1, 0, 0), 1), ((1, 0, 1), 1), ((1, 1, 0), 1), ((1, 1, 1), 0)]
 
def dot_product(values, weights):
  return sum(v * w for v, w in zip(values, weights))
 
while True:
  print('-' * 60)
  error_count = 0
  for input_vector, desired_output in training_set:
    print(weights)
    result = dot_product(input_vector, weights) &gt; threshold
    error = desired_output - result
    if error != 0:
      error_count += 1
      for index, value in enumerate(input_vector):
        weights[index] += learning_rate * error * value
  if error_count == 0:
    break</small>
</pre>
<p>And there we have it, a simple linear classifying single node neural net: the perceptron.</p>
<p><em>The embryo of an electronic computer that the Navy expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.</em> &#8211; The New York Times reporting on the Perceptron (1958)</p>
<h2>Online Learning Perceptron in Python</h2>
<p>We are going to implement the above Perceptron algorithm in Python. We use only standard libraries so the script will run on PyPy (3-4 speedups), taking massive inspiration from tinrtgu&#8217;s online logistic regression script first seen on the Kaggle forums: &#8220;<a href="http://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/10322/beat-the-benchmark-with-less-then-200mb-of-memory">Beat the benchmark with less than 200mb of memory</a>.&#8221;. You can find our script at the <a href="https://github.com/MLWave/online-learning-perceptron">MLWave Github repo</a>.</p>
<h3>Online learning</h3>
<p>The perceptron is capable of online learning (learning from samples one at a time). This is useful for larger datasets since you do not need entire datasets in memory.</p>
<p>Now to steal some feature we&#8217;ve seen in our favorite online machine learning tool &#8212; <a href="http://hunch.net/~vw/">Vowpal Wabbit</a>:</p>
<h4>Hashing trick</h4>
<p>The vectorizing hashing trick originated with Vowpal Wabbit (<a href="http://hunch.net/~jl/projects/hash_reps/">John Langford</a>). This trick sets number of incoming connections to the perceptron to a fixed size. We hash all the raw features to a number lower than the fixed size.</p>
<p>Let&#8217;s take a sample like:</p>
<pre>This movie sucks
</pre>
<p>it may be vectorized with hashing as:</p>
<pre><small>sample = "This movie sucks"
fixed_size = 1024
features = [(hash(f)%fixed_size,1) for f in sample.split()]
print features
</small></pre>
<p>To get the sparse (all non-zero elements) format:</p>
<pre># list of tuples in form (feature_index,feature_value)
&gt;&gt; [(712, 1), (169, 1), (364, 1)]
</pre>
<h4>Progressive Validation Loss</h4>
<p>Learning from samples one at a time also gives us progressive training loss. When the model encounters a sample it first makes a prediction without looking at the target.</p>
<p>Then we compare this prediction with the target label and calculate an error rate. A low error rate tells us we are close to a good model.</p>
<p><a href="http://mlwave.com/wp-content/uploads/2015/01/hal-eye.jpg"><img class="aligncenter size-full wp-image-834" src="http://mlwave.com/wp-content/uploads/2015/01/hal-eye.jpg" alt="hal eye" width="630" height="219" /></a></p>
<p><em>&#8220;No 9000 computer has ever made a mistake or distorted information. We are all, by any practical definition of the words, foolproof and incapable of error.&#8221;</em> &#8212; HAL 9000 (1968)</p>
<h4>Normalization</h4>
<p>Vowpal Wabbit has <a href="http://arxiv.org/abs/1305.6646">online feature value normalization</a>. We use a simple log(feature value + 1) for now and politely bow at the master&#8217;s magnificence.</p>
<h4>Multiple passes</h4>
<p>With Vowpal Wabbit you can run multiple passes over the dataset with a decaying learning rate.</p>
<p>We can also run through datasets multiple times for as long as the error rate gets lower and lower. If the training data is linearly separable then a perceptron should, in practice, converge to 0 errors on the train set.</p>
<p><em>&#8220;By the end, if you make it that far, you&#8217;ll be hoping that the rabbit completes its mission.&#8221;</em> &#8211; User Review for the movie &#8220;The Nasty Rabbit&#8221; (1964)</p>
<h2>Kaggle Results</h2>
<h3>When bag of words meets bags of popcorn</h3>
<p>This is a knowledge and <a href="http://www.kaggle.com/c/word2vec-nlp-tutorial/">tutorial contest</a> on Kaggle. It uses a movie review dataset similar to the one in the paper &#8220;<a href="http://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf">Learning Word Vectors for Sentiment Analysis</a>&#8220;.</p>
<p>The task is to predict if a review has an accompanying rating less than 5 (negative), or over 7 (positive) for 25k test samples. We have a labeled train dataset of 25k samples to build our model.</p>
<p>We write a small function to run through the .TSV datasets and yield the label, identifier, hashed features and feature values.</p>
<h4>Top benchmark</h4>
<p>The current <a href="http://www.kaggle.com/c/word2vec-nlp-tutorial/forums/t/11261/beat-the-benchmark-with-shallow-learning-0-95-lb">best benchmark</a> on the forums is a logistic regression script from <a href="http://www.kaggle.com/users/5309/abhishek">Abhishek</a>. It does an in-memory tfidf-fit_transform on both the train and the test set to get <strong>0.95</strong> AUC on the leaderboard. We skip that transform and stick to our online hashing vectorizer.</p>
<p>We do generate 2-grams (such as &#8220;not good&#8221;) from the features, much like the script from Abhishek does. We simply hash these 2-grams and add them to the sample vectors.</p>
<p>We also quickly clean the text a little by lowercasing and removing anything not alphanumeric.</p>
<h4>Predictions</h4>
<p>To optimize for AUC (the competition metric) we do not submit only &#8220;1&#8243; or &#8220;0&#8243; predictions. This would give us an AUC of around 0.88. We submit the dotproduct normalized between 0 and 1.</p>
<p>Our script matches the solution from Abhishek with a score of <strong>0.952</strong>. But it takes just a few MB of memory, doesn&#8217;t do tfidf transform and converges to 0 errors on train set in just over 2 minutes.</p>
<p>Without cleaning and without 2grams the script runs in less than 60 seconds and produces a score of 0.93.</p>
<pre><small>Pass Errors    Average     Nr. Samples     Since Start
1    5672      0.77312     25000           0:00:04.353000
2    3110      0.8756      25000           0:00:08.464000
3    2280      0.9088      25000           0:00:12.482000
4    1659      0.93364     25000           0:00:16.485000
...
19   138       0.99448     25000           0:01:15.814000
20   110       0.9956      25000           0:01:19.922000
21   83        0.99668     25000           0:01:23.843000
...
29   40        0.9984      25000           0:01:54.922000
30   36        0.99856     25000           0:01:58.802000
31   17        0.99932     25000           0:02:02.607000
32   2         0.99992     25000           0:02:06.521000
33   0         1.0         25000           0:02:10.382000
Zero errors found during training, halting

Testing
Errors    Average    Nr. Samples     Since Start
12431     0.50276    25000           0:00:03.938000
Done testing in 0:00:03.981000</small></pre>
<p><em>&#8220;A recurrent neural works, made his idea to the mentally crucifix &#8216;Cato Green Ronnulae Schill&#8217;&#8221;</em> &#8212; Ilya Sutskever&#8217;s text generating RNN</p>
<h3>ADCG SS14 Challenge 02 &#8211; Spam Mails Detection</h3>
<p>I <a href="http://mlwave.com/winning-2-kaggle-in-class-competitions-on-spam/">already wrote</a> about the <a href="https://inclass.kaggle.com/c/adcg-ss14-challenge-02-spam-mails-detection">spam mail detection challenge</a> before. I gave a lot of credit to the co-competitors who had to write their own classifiers. I am proud to be able to say I now did this competition with code I wrote myself too.</p>
<p>The top result for this competition was my own Vowpal Wabbit solution at <strong>0.998</strong> accuracy. I wrote a small parser to yield the label, id, and features from my old VW datasets.</p>
<p>This online learning perceptron script is able to exactly match the Vowpal Wabbit solution (<strong>0.99836</strong>). Perhaps we have a light-weight Perceptron script that is closer to a Vowpal Wabbit with hinge loss than we dare to think.</p>
<p><em>A simple question: “What exactly is information and data communication?” may be answered using two entirely different theories. The conventional Shannon information theory regards all information as &#8220;quantities&#8221;, which are transmitted using bits and bytes in meaningless bit streams. This outdated theory is still being taught at our universities, even though no biological creature communicates in this way. A biologically correct Autosophy information theory, in contrast, regards all information as &#8220;addresses&#8221;, which are communicated to create new knowledge in a receiver</em>. &#8212; Klaus Holtz (1987)</p>
<h3>Did not work</h3>
<p>All challenges where the data is not pre-normalized between 0-1 gave poor results. This Perceptron script obviously likes sparse one-hot-encoded data, such as text. Also regression and multi-class classification is not implemented yet.</p>
<h3>Code</h3>
<p>You can find the script used for the movie review challenge at the <a href="https://github.com/MLWave/online-learning-perceptron">MLWave Github Repo</a>. Feedback, like adaptive normalized invariance implementations always welcome ;).</p>
<h2>Conclusion</h2>
<p><em>&#8220;For me there is no absolute knowledge: everything goes only by probability&#8221;</em> - Gödel (1961)</p>
<p>A very basic perceptron with 0-1 threshold activation can do well in NLP contests. We did not change the 1957 algorithm that much at all. We could do away with the hashing trick and receive similar or slightly better scores.</p>
<p>This script can not output a probability. One could use a bounded sigmoid on the dotproduct to get a form of online normalization of the output or better study tinrtgu&#8217;s script.</p>
<p>Since a single-node single-layer Perceptron can not model non-linear functions, to get better NLP results we will have go deeper and look at neural network structures like feedforward nets, recurrent nets, self-organizing maps, Multi-layer Perceptrons, word2vec and <a href="http://fastml.com/extreme-learning-machines/">extreme learning machines</a> (fast ffnets without backprop).</p>
<h3>Update: Further Reading</h3>
<p><a href="http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html">Artificial Neurons and Single-Layer Neural Networks &#8211; How Machine Learning Algorithms Work Part 1</a> <i>Thank you Sebastian Raschka</i></p>
<p><a href="http://nautil.us/issue/21/information/the-man-who-tried-to-redeem-the-world-with-logic">The Man Who Tried to Redeem the World with Logic &#8211; Walter Pitts rose from the streets to MIT, but couldn’t escape himself.</a> <i>Thank you Stephen Oates</i></p>
]]></content:encoded>
			<wfw:commentRss>http://mlwave.com/online-learning-perceptron/feed/</wfw:commentRss>
		<slash:comments>34</slash:comments>
		</item>
		<item>
		<title>Lessons learned from the Hunt for Prohibited Content on Kaggle</title>
		<link>http://mlwave.com/lessons-from-avito-prohibited-content-kaggle/</link>
		<comments>http://mlwave.com/lessons-from-avito-prohibited-content-kaggle/#comments</comments>
		<pubDate>Thu, 11 Sep 2014 21:54:16 +0000</pubDate>
		<dc:creator><![CDATA[mladmin]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://mlwave.com/?p=734</guid>
		<description><![CDATA[Previously we looked at detecting counterfeit webshops and feature engineering. Now we will show some progress and learn from our insights (and mistakes) competing in a related Kaggle challenge. Vowpal Wabbit close to for the win Kaggle hosted a contest together with Avito.ru. The task was to automatically detect illicit content in the advertisements on their site. Many competitors [&#8230;]]]></description>
				<content:encoded><![CDATA[<p><strong>Previously we looked at detecting counterfeit webshops and feature engineering. Now we will show some progress and learn from our insights (and mistakes) competing in a related Kaggle challenge.</strong></p>
<h2>Vowpal Wabbit close to for the win</h2>
<p><a href="http://www.kaggle.com/">Kaggle</a> hosted a contest together with <a href="http://www.avito.ru/">Avito</a>.ru. The task was to automatically <a href="https://www.kaggle.com/c/avito-prohibited-content/">detect illicit content in the advertisements</a> on their site.<br />
<span id="more-734"></span></p>
<p>Many competitors were using <a href="http://hunch.net/~vw/">Vowpal Wabbit</a> for this challenge. Some aided by the <a href="https://www.kaggle.com/c/avito-prohibited-content/forums/t/10120/beating-the-benchmark">benchmark</a> from <a href="https://www.kaggle.com/users/28038/foxtrot">Foxtrot</a>, others by starting out the challenge with it. The highest ranking model using VW for a base was <a href="https://www.kaggle.com/users/102203/yr">yr</a>&#8216;s <a href="http://www.kaggle.com/c/avito-prohibited-content/forums/t/10178/congrats-barisumog-giulio/52810#post52810">implementation</a>. This #4 spot used the benchmark provided by Avito as part of the pipeline.</p>
<p>Our team (<a href="http://julesvanligtenberg.nl/">Jules van Ligtenberg</a>, <a href="https://www.kaggle.com/users/115173/phil-culliton">Phil Culliton</a> and me, <a href="https://www.kaggle.com/users/114978/triskelion">Triskelion</a>) ended up in 8th place with an average precision of ~0.985. A team of Russian moderators had an average precision of ~0.988 when labeling the dataset. Our team did not speak Russian, just English, Dutch and <a href="http://en.wikipedia.org/wiki/MurmurHash">MurmurHash</a>.</p>
<blockquote><p>It is truly amazing that so many international teams that have no knowledge of Russian language made it to the top. <cite>Ivan Guz &#8211; Competition Admin</cite></p></blockquote>
<h2>Insights</h2>
<h3>What did work</h3>
<ul>
<li><strong>Ensembling Vowpal Wabbit models</strong>. By simply averaging the ranks of different submission files one could up the score. Combining a squared, logistic and hinge loss model this way gave a score of ~0.982, while all individual models scored around ~0.977.</li>
<li><strong>Using an illicit score.</strong> This changes the problem from classification to regression. Instead of training models on labels of [illicit, non-illicit], we used the provided &#8220;closing hours&#8221; and &#8220;is proved&#8221; variables to create an &#8220;illicit score&#8221;. The worst offenders for this model are ads that are &#8220;blocked&#8221; by a moderator, &#8220;proved&#8221; by an experienced moderator, and &#8220;closed&#8221; within minutes of being published on the site.</li>
<li><strong>All loss functions</strong> gave good results. Initially we gravitated towards logistic loss and hinge loss. Later we added a squared loss and a quantile loss. For example averaging the ranked outputs of both a logistic and a hinge loss model, with all the parameters and data the same, gave a ~0.003 increase in score. We will study these &#8220;hybrid&#8221; loss functions better.</li>
<li><strong>Neural networks</strong>. A <a href="http://www.machinedlearnings.com/2013/02/one-louder.html">functionality added to VW</a> with the motive to win some Kaggle competitions (Thank you! One louder, indeed.). This feature gave a nice and welcome non-linear boost of ~0.001 at the end with <code>--nn 10</code>.</li>
<li><strong>Reducing overfit</strong>. By averaging all best performing ensemble models we created a model with a lower score on the leaderboard, but with less variance between public and private leaderboard. We did not hit the point of diminishing returns on leaderboard improvements. During the competition 9th place was our highest position on the public leaderboard.
<p><iframe width="474" height="267" src="https://www.youtube.com/embed/tleeC-KlsKA?feature=oembed&#038;start=1114" frameborder="0" allowfullscreen></iframe></p>
<p><i><small><a href="https://www.kaggle.com/users/993/ben-hamner">Ben Hamner</a> explains overfitting to the Kaggle leaderboard and provides some insights.</small></i></li>
<li><strong>2-grams on ALL the features</strong>. I have no idea why this works. It&#8217;s still a bit unclear, because this is unconventional: it makes little sense. Perhaps it works like a lesser form of quadratic features. Perhaps it works because there is text crudely mixed between the features. Perhaps VW is really THAT good in ignoring irrelevant features. Or perhaps it works because spreadsheet and dataset creators put semantically related columns close to another.</li>
<li><strong>Having access</strong> to a fast 32GB RAM machine. One of the team members was able to quickly train and inspect Vowpal Wabbit models with a huge bitsize (2^30). Less collisions usually (but not always) works better for learning.</li>
<li><strong>Encoding integers</strong> in categorical variables. For example: year_2009 and year:2009.</li>
</ul>
<h3>What did not (quite) work</h3>
<ul>
<li><strong>Hyperparameter tuning.</strong> We did not set up a pipeline with cross-validation and model evaluation according to the competition&#8217;s metric. Parameters were tweaked with modesty, based on slightly worried hunches.</li>
<li><strong>TF-IDF</strong>. We suspected that TF*IDF would improve the score. To fit a TF*IDF filter on both the train and test set and replace all datasets with properly namespaced features proved too cumbersome/complex.</li>
<li>Quick <strong>character encoding</strong> handling. It took me far too long to barely get this working, and then I started over, scratching the benchmark code completely, never improving on it. Turning Cyrillic characters into Latin characters did help, but is a dirty workaround.</li>
<li>Proper <strong>dataset inspection</strong>. All the column headers and variables were in a language our team did not speak. All feedback on model performance was leader board- driven. I initially missed 2.5 million lines in the train set (later more on that).</li>
<li><strong>Bagging SVD&#8217;s</strong>. Though it could beat Avito&#8217;s own benchmark at around ~0.925, with ~0.952 these models did not contribute to the final ensemble.</li>
</ul>
<p><img class="aligncenter size-full wp-image-745" src="http://mlwave.com/wp-content/uploads/2014/09/carter-murmur.jpg" alt="Carter swamp rabbit" width="498" height="158" /></p>
<p><i><small>President Carter confessed to having limited experience with Vowpal Wabbits, preferring to stick with R.</small></i></p>
<h3>What could have worked</h3>
<ul>
<li><strong>Nearest neighbours</strong>. <a href="https://www.kaggle.com/users/3090/alexander-d-yakonov">Alexander D&#8217;yakonov</a> combined Nearest neighbors (120 neighbors, weights based on distance) and a basic Vowpal Wabbit model combined to rank #5.</li>
<li><strong>Factorization machines</strong>.  <a href="https://www.kaggle.com/users/1455/michael-jahrer">Michael Jahrer</a> and <a href="http://www.kaggle.com/users/96423/mikhail-trofimov">Mikhail Trofimov</a> used factorization machines to rank over 0.98</li>
<li><strong>SVC</strong>. The winners, <a href="https://www.kaggle.com/users/111776/giulio">Giulio</a> and <a href="https://www.kaggle.com/users/94510/barisumog">Barisumog</a>, report using SVC successfully.</li>
<li><strong>Random Forests</strong>. With it&#8217;s trackrecord as one of the most powerful algorithms in machine learning: RF&#8217;s working is probably a given. Our best exploratory model (useful to spot good features etc.) used sklearn&#8217;s <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">Random Forests</a> too, albeit for a more moderate score of ~0.805.</li>
<li><strong>TF-IDF</strong>. Nearly everyone in the top 10 had <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">tfidf-vectorized</a> their datasets.</li>
<li>Using <strong>Avito&#8217;s provided benchmark</strong>. It contained both domain knowledge and a few very specific tricks in preprocessing the data.</li>
<li>Training <strong>models for each category</strong>. Trading increased complexity for increased predictive powers.</li>
</ul>
<h2>Ease of implementation</h2>
<p><img class="alignleft size-thumbnail wp-image-743" src="http://mlwave.com/wp-content/uploads/2014/09/800px-Robin_Hood_Memorial-150x150.jpg" alt="Robin Hood Statue" width="150" height="150" />I very much agree with FastML&#8217;s article on <a href="http://fastml.com/kaggle-vs-industry-as-seen-through-lens-of-the-avito-competition/">this competition vs. the industry</a>. With the industry it is enough to hit the (often moving) target, and profitable to hit the bullseye. With Kaggle one is splitting arrows.</p>
<h3>Vowpal Wabbit vs. the industry</h3>
<p>Solutions based on Vowpal Wabbit would work well enough for Avito, or for any big moderator labeled dataset for that matter.</p>
<p>Though even with Vowpal Wabbit and basic techniques caution is required.</p>
<ul>
<li>Using an ensemble of 10 different Vowpal Wabbit models, means running 10 instances of Vowpal Wabbit if you want a real-time prediction.</li>
<li>Train a specific model for every category and a site with 1000+ categories will go crazy.</li>
<li>TF*IDF combined with retraining on new data adds quite a preprocessing step and increased complexity.</li>
</ul>
<p>Highly tuned single Vowpal Wabbit models approach 0.98. Averaging the outputs from two moderately inspired Vowpal Wabbit models gets one comfortably in the top 10% range and near the top 10 leaderboard.</p>
<h3>Bag-of-features</h3>
<p>The dataset had a column (attributes) which contained a JSON object. We really wanted to create tidy features from these, but to rely on Google Translate for feature engineering was too time-consuming. We threw everything the script could parse into one bag of &#8220;features&#8221;, mixing numerical, categorical and text features.</p>
<pre>
1 '10000074 |f category_x_transport emails_cnt:0.0 emails_cnt_x_0 avtomobil_ v ideal_nom sostoanii exclamationmark 2005 goda dekabr_ vse detali rodnye dva hozaina nikakih vlojenij ne trebuet komplektazia polnaa kondizioner gur perednie steklo pod_emniki 2 poduski frontal_nye vse rabotaet otlicno signalizazia s obratnoj svaz_u muzyka mr3 lubye proverki za vas scet exclamationmark exclamationmark exclamationmark renault logan 2005 price:205000.0 price_x_205000 phones_cnt:0.0 phones_cnt_x_0 urls_cnt:0.0 urls_cnt_x_0 ob_em_dvigatela:1.6 ob_em_dvigatela_x_1_6 model__x_logan marka_x_renault tip_dvigatela_x_benzinovyj korobka_peredac_x_mehaniceskaa probeg_x_180_000_189_999 sostoanie_x_ne_bityj rul__x_levyj tip_kuzova_x_sedan zvet_x_seryj privod_x_perednij god_vypuska:2005.0 god_vypuska_x_2005 subcategory_x_avtomobili_s_probegom
</pre>
<p><small><i>First line from Vowpal Wabbit&#8217;s test set</i></small></p>
<p>Using this data agnostic approach and very little to no feature engineering, one can use Vowpal Wabbit to get good scores. If you have a good moderator labeled dataset, but no good solution yet, contact me or leave a message: our team would love to keep working on such datasets.</p>
<p>In short, we did not treat the data or Vowpal Wabbit with much respect at all. We threw millions of men at the Wabbit and it left only a cave surrounded by bones.</p>
<p><img class="aligncenter size-full wp-image-744" src="http://mlwave.com/wp-content/uploads/2014/09/Rabbitattack1.jpg" alt="Killer Rabbit Attack" width="400" height="221" /></p>
<h2>How I forgot ~2.5 million rows and almost got away with it.</h2>
<p>It took me a long time to join the competition, because I couldn&#8217;t get the benchmark running. Normally a lot of inspiration and momentum comes from running or recreating the benchmark. I quickly became team UnicodeEncodeError.</p>
<p>I&#8217;ve worked with European languages, which do have their fair share of diacritics and other arcane symbols, but Windows + The Python Benchmark + Russian text equalled zero for me.</p>
<p>When I did finally submit my first VW predictions I got a score of around ~0.971. By (incorrectly) answering a <a href="https://www.kaggle.com/c/avito-prohibited-content/forums/t/9953/can-t-read-after-1562936th-row-in-training-data">question by yr</a> on the forums, I finally found out that the dataset when read on Windows produced around 1.5 million lines, and when read with Pandas or on other platforms would give the full size. Note to self: Keep writing files with &#8220;wb&#8221;-mode, start reading files with &#8220;rb&#8221;-mode.</p>
<h3>Wanting to learn (without making mistakes)</h3>
<p>According to <a href="https://www.kaggle.com/users/4398/sergey-yurgenson">Sergey Yurgenson</a> there are at least three types of Kagglers:</p>
<ul>
<li>Those who want to learn,</li>
<li>those who want to win money,</li>
<li>those who want to increase their reputation.</li>
</ul>
<p>Up to this point I was clearly in the want-to-learn camp. I had nothing to lose by competing and making rookie mistakes. But now I start to feel bad when I make a fool out of myself with such basic mistakes.</p>
<p>I&#8217;d still gladly find these things out. If that be publicly on the forums, though a bit shameful, so be it. If I had teamed up earlier (or used more than one OS) I probably would have found this out sooner.</p>
<p>I realize that in Kaggle competitions one may be disrespectful of the context (domain knowledge) of the data to a degree, but one should always respect the syntax. Data inspection (measuring data quality) should be an essential part of the pipeline.</p>
<h2>So how about those counterfeit webshops?</h2>
<p>The problem is that I would want to get a good result, but have to create my own dataset for this. I can not remove this prior belief that Machine Learning can combat online illicit and scam content, so I am afraid I will fall prey to a subtle form of overfit.</p>
<blockquote>
<ul>
<li>Choose the best of Accuracy, error rate, (A)ROC, F1, percent improvement on the previous best, percent improvement of error rate, etc.. for your method. For bonus points, use ambiguous graphs.</li>
<li>Chose to report results on some subset of datasets where your algorithm performs well.</li>
<li>Alter the problem so that your performance improves.</li>
<li>After a dataset has been released, algorithms can be made to perform well on the dataset using a process of feedback design, indicating better performance than we might expect in the future. Some conferences have canonical datasets that have been used for a decade.</li>
</ul>
<p><cite><a href="http://hunch.net/?p=22">John Langford (2005) &#8211; Subtle Methods of Overfitting</a></cite></p></blockquote>
<p>I realized that what I will be making is something to solve a very specific problem: Find out how I gathered and labeled my dataset. To do this correctly I would need a way to realistically reproduce a new test set, but one that is created one week after I created my model, preferably by real-life users of the model.</p>
<p>In short, only a model in production can prove its worth. To get a glimpse of web-scale anti-spam measures read this <a href="https://moderncrypto.org/mail-archive/messaging/2014/000780.html">inside story</a> from the trenches by a 7-year Google engineer.</p>
<blockquote><p>In the beginning &#8230; there was the regex. Gmail does support regex filtering but only as a last resort. It&#8217;s easy to make mistakes, like the time we accidentally blackholed email for an unfortunate Italian woman named &#8220;Oli*via Gra*dina&#8221;. Plus this technique does not internationalise, and randomising text to miss the blacklists is easy.</p></blockquote>
<p>I&#8217;ll be on the lookout for more well-published datasets in this space, to compare my approaches with others. If your site creates a lot of data and faces a similar problem of spam and illicit content, contact me or leave a message, I&#8217;d love to chat with you.</p>
<h2>Further reading</h2>
<ul>
<li>Post-competition <a href="http://www.kaggle.com/c/avito-prohibited-content/forums/t/10178/congrats-barisumog-giulio">approach sharing thread</a> on the Kaggle forums.</li>
</ul>
<h3>Ensembling code</h3>
<pre>
from glob import glob
from collections import defaultdict

def kaggle_rank_avg(glob_files,loc_out):
  """
    Averaging multiple submission files for 
    Kaggle's "Hunt for Prohibited Content"
    Enter location to submission files
    Get a new submission file on loc_out
  """
  ranks = defaultdict(float)
  with open(loc_out,"wb") as outfile:
    print(glob_files)
    for i, glob_file in enumerate(glob(glob_files)):
      print(glob_file)
      for e, line in enumerate(open(glob_file,"<b>rb</b>")):
        if i == 0 and e == 0:
          outfile.write(line)
        if e > 0:
          ranks[line.strip()] += e
    for k in sorted(ranks, key=ranks.get):
      outfile.write("%s\n"%(k))
#kaggle_rank_avg("d:\\avito\\*.csv", "d:\\avito.ensemble.csv")
</pre>
<p><small>Images were from wikimedia commons (authors <a href="http://en.wikipedia.org/wiki/User:Soerfm">Soerfm</a>, <a href="http://commons.wikimedia.org/wiki/User:Mousse">Mousse</a> and <a href="http://en.wikipedia.org/wiki/User:Sven_Manguard">Sven Manguard</a>). The intro image is from a <a href="https://www.youtube.com/watch?v=NjkbGoJD4PE">commercial</a> from Avito.ru and the photo of president Carter refusing refuge to a Vowpal Wabbit was given to me by a man in a trench-coat inside a poorly lit parking lot.</small></p>
]]></content:encoded>
			<wfw:commentRss>http://mlwave.com/lessons-from-avito-prohibited-content-kaggle/feed/</wfw:commentRss>
		<slash:comments>9</slash:comments>
		</item>
		<item>
		<title>yCombinator 2014 Data Science Start-ups</title>
		<link>http://mlwave.com/ycombinator-2014-data-science-start-ups/</link>
		<comments>http://mlwave.com/ycombinator-2014-data-science-start-ups/#comments</comments>
		<pubDate>Fri, 29 Aug 2014 12:07:36 +0000</pubDate>
		<dc:creator><![CDATA[mladmin]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://mlwave.com/?p=674</guid>
		<description><![CDATA[There are new and exciting commercial opportunities in the data science space. We take a look at the data science start-ups from the latest yCombinator batch. Niches Customer analytics Framed Data. Framed Data is a good example of a new data science company. Using machine learning to predict customer churn. An attractive problem to attack, since [&#8230;]]]></description>
				<content:encoded><![CDATA[<p><strong>There are new and exciting commercial opportunities in the data science space. We take a look at the data science start-ups from the latest yCombinator batch.</strong></p>
<h2>Niches</h2>
<h3>Customer analytics</h3>
<p><span id="more-674"></span></p>
<p><a href="http://www.framed.io/"><strong>Framed Data</strong></a>.<img class="alignright size-thumbnail wp-image-698" src="http://mlwave.com/wp-content/uploads/2014/08/framed_logo1-150x150.png" alt="Framed Data" width="150" height="150" /> Framed Data is a good example of a new data science company. Using machine learning to predict customer churn. An attractive problem to attack, since no company wants to lose customers.</p>
<p>We wish them much luck with their bigger mission: To turn all companies into data-informed organizations.</p>
<p><a href="https://nextcaller.com/"><strong>NextCaller</strong></a>. NextCaller is an advanced caller identification platform. If a caller matches a record in your CRM, that profile is shown on-demand. If the caller does not match a record in your CRM, a general profile will be shown, made from the database of all clients.</p>
<p>Sharing a CRM like this with many other companies is a huge benefit: one makes use of far more data. Exciting to see how this model/approach can be adapted for other businesses.</p>
<p><a href="https://taplytics.com/"><strong>Taplytics</strong></a>. Taplytics promises a fully-featured A/B testing platform for mobile apps. Use their new dashboard to receive stats on: acquisition performance, user engagement and user retention.</p>
<p>Mobile app developers will want to A/B test their apps. If they don&#8217;t they are leaving food on the table. Taplytics is an attractive platform for developers who do not want to worry about implementing A/B testing on their own.</p>
<h3>Information retrieval</h3>
<p><a href="https://www.algolia.com/"><strong>Algolia</strong></a>. Algolia is an API to create search engines for any site. Powerful tools enable developers to create search engines in a few clicks. yCombinator&#8217;s internal search is powered by Algolia. We&#8217;ve tried their API, but found their results to be a little too fuzzy for our purpose. Fuzziness can of course be a strength: an intelligent search engine will also search for semantically and syntactically related terms. For NLP tasks we prefer stricter results.</p>
<p>Many searchers already use Google to navigate websites. Where internal search could shine is in customization and personalization: A webshop could rank more profitable products higher, or target the results to the searcher&#8217;s purchase history.</p>
<h3>Online marketing</h3>
<p><a href="http://www.boostable.com/"><strong><img class="alignright size-thumbnail wp-image-701" src="http://mlwave.com/wp-content/uploads/2014/08/tumblr_n1418zpXPL1ri7brgo1_12801-150x150.png" alt="Boostable" width="150" height="150" />Boostable</strong></a>. Boostable makes online advertising easy and effective for anyone. Connect your store and set up your preferences, and your ads will automatically be shown to the right customers.</p>
<p>Improving conversion rates directly shows itself in improved profits. As long as Boostable can keep showing uplifts in visitors and sales, it will remain an interesting value proposition to advertisers.</p>
<p><a href="https://www.orankl.com/"><strong>Orankl</strong></a>. Orankl connects reviews with marketing emails. With very little effort you can add user tracking and review forms to your website. This information is then processed to generate marketing emails with product recommendations tailored to each user.</p>
<p>Personalization is the next step for conversion optimization. Instead of optimizing for everyone (which is bound to fail for some), optimize for each user&#8217;s data profile.</p>
<p><a href="https://www.sendwithus.com/"><strong>SendWithUs</strong></a>. SendWithUs integrates your email service providers and has built-in support for A/B tests and extensive analytics.</p>
<p>Where SendWithUs distinguishes itself from other mail providers is by providing support for drip campaigns and allowing the marketer to manage email campaigns without the aid of a developer.</p>
<p><a href="https://stacklead.com/"><strong>StackLead</strong></a>. StackLead connects multiple datasources to give you insight in your new customers. From just a name and an e-mail address they try to find data and metrics like: industry, title, number of employees.</p>
<p>If use of StackLead leads to closing more deals and warmer leads, they&#8217;ll be in business for a very long time.</p>
<h3>Data dashboards</h3>
<p><a href="https://www.abacus.com/"><strong><img class="alignleft size-thumbnail wp-image-702" src="http://mlwave.com/wp-content/uploads/2014/08/mobile1-150x150.png" alt="Abacus" width="150" height="150" />Abacus</strong></a>. Abacus solves the hassle around business expenses. Soon after expenses are approved the employee will get paid. Managers and accountants get an overview of spending by employee, category, project, vendor and location. Syncing with bookkeeping software is also possible.</p>
<p>We think Abacus is likely to disrupt the back office. It solves an inefficiency and benefits both the employers and employees.</p>
<p><a href="https://www.rocketrip.com/"><strong>Rocketrip</strong>.</a> With Rocketrip you reduce spending by rewarding employees for saving. Combining custom settings and real-time market data, employees can book hotels and flights, or rent cars, all within the companies budget.</p>
<p>Much like Abacus, the value for a business is clear: less hassle and saved expenses.</p>
<p><a href="https://www.ambition.com/"><strong>Ambition</strong></a>. Ambition is a start-up in company metrics. It integrates a wide variety of platforms (phone, spreadsheet, CRM). With it&#8217;s own Ambition metric it aims to capture employee productivity metrics.</p>
<p>An ambitious problem to attack with a lot of potential for growth.</p>
<p><a href="https://42technologies.com/"><strong>42Technologies</strong></a>. 42Technologies is the broadest data science application of this batch. It offers data analysis for retailers, with a focus on: recommendations, performance, customer analytics, growth, and ROI optimization.</p>
<p>With a great team and a lot of dedication to let retailers make better data-informed decisions, we have little doubt that they&#8217;ll make a dent.</p>
<h3>Tracking &amp; Analytics</h3>
<p><a href="https://www.bellabeat.com/"><strong><img class="alignright size-thumbnail wp-image-703" src="http://mlwave.com/wp-content/uploads/2014/08/169012-c22bc5d9024a4cdff83383ddde2ddb6a-medium_jpg1-150x150.jpg" alt="Bellabeat" width="150" height="150" />Bellabeat</strong></a>. Bellabeat&#8217;s hook is to use your phone to listen to your baby&#8217;s heartbeat. After that it keeps their users engaged tracking their baby&#8217;s movement and pregnancy weight gain.</p>
<p>A good example of a niche where data applications can thrive. We don&#8217;t expect this market to be a passing fad. People will want statistics about every aspect of their lives, including their pregnancies.</p>
<p><strong><a href="https://piinpoint.com/">PiinPoint</a>.</strong> PiinPoint enables businesses to find the best locations for expansion. A clear value proposition for researching the markets and their competitors.</p>
<p>In a market with rich customers, if Piinpoint manages to onboard new users, they stand to make a lot of money for themselves and their customers.</p>
<p><a href="http://www.terravion.com/"><strong>Terravion</strong></a>. Terravion lets farmers subscribe to aerial photography: Select just the patches of land you want, and their airplane will start to take pictures. All data is conveniently available in an online dashboard.</p>
<p>Another company, The Climate Corporation, has already proven that huge buy-outs are possible in this market. Just securing a small slice of the pie will be hugely profitable.</p>
<h3>Healthcare</h3>
<p><a href="http://caremessage.org/"><strong> <img class="alignleft size-thumbnail wp-image-705" src="http://mlwave.com/wp-content/uploads/2014/08/caremessage-150x150.jpg" alt="CareMessage" width="150" height="150" />CareMessage</strong></a>. CareMessage offers simplified care management for hospitals and doctors. Use their dashboards to automate reminders, send personalized messages and start tailored programs.</p>
<p>We find these evidence-based health education programs very interesting: replicating the experience of interacting with a live health coach with a digital app.</p>
<p><a href="https://www.immunityproject.org/"><strong>Immunity Project</strong></a>. Immunity Project wins this list for the most noble goal: End HIV/AIDS and offer the cure for free. Talk about &#8220;making the world a better place&#8221;.</p>
<p>Their statistical analysis shows promising research opportunities. Let&#8217;s hope that the cure for AIDS becomes the biggest victory of data science to date.</p>
<p><a href="https://www.truevault.com/"><strong>TrueVault</strong></a>. With TrueVault one can store medical data in a HIPAA compliant manner.</p>
<p>TrueVault targets healthcare app developers. Working with and storing data becomes tricky when legal issues arise. Data security and data provenance will only become bigger topics in the future, allowing TrueVault plenty of opportunity for growth (perhaps also in other niches)</p>
<h3>Data Mining</h3>
<p><a href="https://www.kimonolabs.com/"><strong><img class="alignright size-thumbnail wp-image-706" src="http://mlwave.com/wp-content/uploads/2014/08/chrome-extension-2561-150x150.png" alt="Kimono Labs" width="150" height="150" />Kimono Labs</strong></a>. Kimono Labs turns website&#8217;s into API&#8217;s with a visual editor and a few clicks. No more messy scraping, with Kimono Labs one does not need to write any code.</p>
<p>Allowing customers to quickly get to the data they want is a great feature. Intelligent pattern extraction saves a lot of time, both for the novice and expert.</p>
<h3>Match-making</h3>
<p><a href="http://thedatingring.com/"><strong>The Dating Ring</strong></a>. A standard dating site with a twist: using matchmakers to set up dates. The Dating Ring makes extensive use of algorithms for matching.</p>
<p>Fixing online dating was one of the ambitious <a href="http://old.ycombinator.com/ideas.html">start-up ideas</a> that YC posed. Any site that can overcome the chicken-egg problem is a force to be reckoned with.</p>
<h3>Finance</h3>
<p><a href="https://tradeblock.com/"><strong><img class="alignright size-thumbnail wp-image-709" src="http://mlwave.com/wp-content/uploads/2014/08/tradeblock-150x150.png" alt="Tradeblock" width="150" height="150" />TradeBlock</strong></a>. TradeBlock jumps into the BitCoin fray with a data analysis platform for digital currencies. TradeBlock offers dashboards to track mining, markets and research investing opportunities.</p>
<p>A no-brainer for YC. Digital currencies are a new phenomenon and this market definitely benefits from data. TradeBlock could become the final end-point for all statistics and analytics related to digital currencies.</p>
<p><a href="https://www.zidisha.org/"><strong>Zidisha</strong></a>. Zidisha will use data science to improve its micro-financing platform. The non-profit Zidisha employs another YC startup, <a href="http://www.bayesimpact.org/">Bayes Impact</a>, to make use of this data.</p>
<blockquote><p>&#8220;We have enough data so that we can use data science to develop an algorithm to predict not just fraud but also credit risk.&#8221;</p></blockquote>
<h3>Machine Translation &amp; NLP</h3>
<p><a href="https://www.unbabel.com/"><strong><img class="alignleft size-thumbnail wp-image-710" src="http://mlwave.com/wp-content/uploads/2014/08/photo2-150x150.png" alt="Unbabel" width="150" height="150" />Unbabel</strong></a>. Unbabel brands itself as &#8220;translation as a service&#8221;. Combining machine translation with human translators they get better and faster results.</p>
<p>Adding the human element to machine learning is interesting. Instead of leaving it all up to some black-box algorithm, Unbabel places machine learning in service of the humans.</p>
<p><strong><a href="https://wit.ai/">Wit.ai</a>.</strong> Wit.ai offers natural language for the internet of things. In short it wants to answer questions posed in natural language, such as &#8220;what is the weather like tomorrow in SF?&#8221;, turning it into intent, parsing the locations and finding the right forecast.</p>
<p>As devices become smaller and smaller, and our need to use computers to remember facts increases, so does the need for an intelligent platform that understands natural language. In combination with improved speech-to-text in the near future we may wonder how we ever used those tiny keyboards.</p>
<h2>Job Skills</h2>
<p>What skills are these data science start-ups looking for? We look at their open job vacancies and see skills like:</p>
<ul>
<li>Data visualization</li>
<li>Machine learning</li>
<li>Distributed systems</li>
<li>Familiarity with compliance &amp; security standards including PCI DSS, FFIEC, GLBA, ISO 27001, HIPAA, and NIST</li>
<li>Well versed in JSP, JavaScript, JSON, XML (VXML &amp; CCXML)</li>
<li>Experience with source code control tools</li>
<li>2+ years of Java experience</li>
<li>Fluency in Python, Java, C++, or similar (Python strongly preferred)</li>
<li>Production experience with relational databases</li>
<li>Experience with distributed caching techniques</li>
<li>Solid foundation in data structures, algorithms and complexity analysis</li>
<li>Strong programming background in Linux</li>
<li>Passion for security, and a practical and balanced approach to security issues</li>
<li>Familiarity with AWS and MySQL</li>
<li>A knack for solving complex UI &amp; UX problems</li>
<li>Experience building your own MEAN apps</li>
<li>Expertise in building clean, api driven code</li>
<li>Optimizing marketing strategies based on performance metrics.</li>
<li>from backend Python services to slick dashboard features in JavaScript.</li>
<li>Advise clients on strategies to meet their marketing objectives.</li>
<li>You’ve built and launched your own projects.</li>
<li>You have a Github account and you read Hacker News</li>
<li>You know what it means to build lean and iterate.</li>
<li>Node/Express</li>
<li>AWS</li>
<li>MySQL</li>
<li>AngularJS</li>
<li>Objective-C</li>
<li>Java Android SDK</li>
<li>5 years of relevant work experience, including large systems software design and development experience, with knowledge of UNIX/Linux.</li>
<li>A Polyglot with experience applying multiple web development languages to live applications.</li>
<li>well-versed in a Python web stack</li>
<li>strong UI development experience using HTML, CSS and JavaScript/AJAX.</li>
<li>a solid foundation in computer science, you have strong competencies in data structures, algorithms, and software design.</li>
</ul>
<p><small>The intro image is in the public domain and depicts Paul Graham speaking to a new batch of YC companies.</small></p>
]]></content:encoded>
			<wfw:commentRss>http://mlwave.com/ycombinator-2014-data-science-start-ups/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Detecting Counterfeit Webshops. Part 1: Feature engineering</title>
		<link>http://mlwave.com/detecting-counterfeit-webshops-part-1-feature-engineering/</link>
		<comments>http://mlwave.com/detecting-counterfeit-webshops-part-1-feature-engineering/#comments</comments>
		<pubDate>Tue, 05 Aug 2014 13:19:06 +0000</pubDate>
		<dc:creator><![CDATA[mladmin]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://mlwave.com/?p=466</guid>
		<description><![CDATA[The number of fake webshops is rising. From 2010 to 2012 the Dutch authority on internet scams received 81.000 complaints. Spammers have moved from running their own webshops to hacking websites or registering expired domain names. This makes classification more difficult. In this series we will experiment with machine learning to automatically classify the trustworthiness [&#8230;]]]></description>
				<content:encoded><![CDATA[<p><strong>The number of fake webshops is rising. From 2010 to 2012 the Dutch authority on internet scams received 81.000 complaints. Spammers have moved from running their own webshops to hacking websites or registering expired domain names. This makes classification more difficult.</strong></p>
<p>In this series we will experiment with machine learning to automatically classify the trustworthiness of a webshop (and by extension, any malicious website). The focus will be on fake webshops hosted on the Dutch TLD (.nl) or catering to Dutch users.</p>
<p>Update: <a href="https://googleonlinesecurity.blogspot.nl/2015/09/new-research-underground-market-fueling.html">New Google Research: The underground market fueling for-profit abuse</a>.</p>
<p><iframe width="474" height="267" src="https://www.youtube.com/embed/sZ-RWIy7dkw?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<p><span id="more-466"></span></p>
<h2>Understanding the problem</h2>
<p>Identifying counterfeit webshops has transformed from a spam problem into a crime, economy, health and security problem.</p>
<p>Consumer safety at a webshop relies on:</p>
<ul>
<li>The technical safety of the webshop itself (some webshops are vulnerable to hacking, yet handle your credit card or banking information)</li>
<li>The trustworthiness of the webshop owner (does he or she deliver as promised?)</li>
<li>The safety of the payment process.</li>
</ul>
<h3>Fake online pharmacies</h3>
<p>11% of the adult Dutch people order medicine and pills online (Abortion pills, Cialis, Viagra, Xanax). The chance of these pills being fake is over 50%. Fake pills can cause medical risks (a Dutch government organization found rest products of XTC in confiscated scam medicine).</p>
<p>Producers and webshop of counterfeit medicine become more professional. Often the average internet visitor can not distinguish fake from real. The payment process is smooth and the delivered packages look real and trustworthy.</p>
<p>When the Ministry of Health, Welfare and Sport put up a fake pharmacy on medi-plaza.nl after a few months 18.000 people had placed an order (Customers received a warning and education flyer).</p>
<h3>Analysing a current scam</h3>
<p>Scammers target users on marktplaats.nl (the Dutch eBay). They hack accounts from webshop owners or they convincingly copy (fork) these accounts. They then place copied advertisements or listings which redirect to a scam webshop on a domain they control. The scam webshop may copy the contact information and graphics of the victim webshop.</p>
<p>Then they wait for orders to come in, yet will never deliver. Money is deposited in the bank accounts of lackeys: The (often underage) lackeys receive moderate payment for providing access to their accounts, and have no clue what is really going on, until they are arrested when the trail leads to them.</p>
<h3>Prevention guidelines</h3>
<p>There are a few expert guidelines to manually spot fake webshops:</p>
<ul>
<li><strong>G1)</strong> Search Google for reviews and complaints in combination with the name of the webshop.</li>
<li><strong>G2)</strong> Search the official registers to spot if the webshop is registered and if the data is correct and up-to-date.</li>
<li><strong>G3)</strong> Check the product pricing and see if it conforms to the standard market price. Be more careful with overtly cheap pricing.</li>
<li><strong>G4)</strong> Check the terms of service and delivery policy: Does the webshop demand that you pay more than 50% up front? Are the return policies fair? Do you get your money back in case of voiding the purchase? Do you know when the product is delivered?</li>
<li><strong>G5)</strong> Are there spelling errors on the site?</li>
<li><strong>G6)</strong> Is there a phone number or address to contact the site owner?</li>
<li><strong>G7)</strong> Check if the product is legal: Can you buy the medicine without prescription? Is it legal to own that BB-gun?</li>
</ul>
<p>We will try to capture these guidelines into features.</p>
<h3>(Business) value of an effective classifier</h3>
<ul>
<li>Brand owners fighting trademark infringement.</li>
<li>Search engines cleaning their index and protecting users and webmasters.</li>
<li>Webshops cleaning their own sites from spam reviews and detecting anomalous pricing.</li>
<li>Trading platforms to detect scammers and scam websites.</li>
<li>Browser vendors to their protect users.</li>
<li>For direct consumers awareness in the form of a browser plug-in.</li>
<li>For better research into the domain and coordinated action to better solve it.</li>
</ul>
<h2>Guidelines-based Features</h2>
<h3>G1) &#8220;Query Google&#8221;-type features</h3>
<p>As automatically quering Google is against their Terms of Service, and the Custom Search Engine API would not suffice (due to differing results and 100 queries a day), querying Google to generate features becomes hard.</p>
<p>In an ideal world we would generate features for the normalized number of search results for the webshop name in relation to terms like: &#8220;scam&#8221;, &#8220;not trust&#8221;, &#8220;negative review&#8221;. Also we&#8217;d prefer a better estimate of Google PageRank, links and domain authority.</p>
<h4>Google features</h4>
<p>None for now.</p>
<h4>Complaint features</h4>
<p>An alternative would be to create a list of review and complaint sites. Though more manual work with an API or scraper, this would offer the benefit of a fine-grained search on the sites, and to possibly calculate average votes.</p>
<p>A related problem to this is the problem of fake reviews. Long-term scamming webshops may insert fake positive reviews. These are harder for an outsider to spot (one would usually need more insider information, like IP).</p>
<p>An example of a complaint site with data on webshops is <a href="http://www.opgelicht.nl/alerts/webshops/">opgelicht.nl</a>. Complaints range from violations of the law on personal privacy (compare CAN-SPAM act) to full scam websites using stolen credentials.</p>
<p>A basic binary feature would be:</p>
<pre>complaint_site_opgelicht_nl_warns_against:1</pre>
<p>A basic count feature for another <a href="http://www.opgeletopinternet.nl/index.php/board,5.0.html">complaint site</a> would be:</p>
<pre>complaint_site_opgeletopinternet_nl_warns_against:7</pre>
<p>where 7 is the number of pages of a complaint thread. To realize this feature I wrote a web crawler with <a href="http://swizec.com/blog/scraping-with-mechanize-and-beautifulsoup/swizec/5039">Mechanize and BeautifulSoup</a>. Crawling and storing around 1000 (often already offline) problem URL&#8217;s.</p>
<p>Data from <a href="https://www.mijnpolitie.nl/if.shtml">mijnpolitie.nl</a> (The central point to report internet scams to Dutch justice) could serve well as features, but this data is not (yet) publicly accessible for automatic querying.</p>
<h4>Review Features</h4>
<p>A webshop being listed on a review site (even with zero reviews) should be a decent signal for its legitimacy. Most scam webshops do not last long enough to bother with reviews and review sites.</p>
<p>I gathered the URLs of around 10.000 unique webshops together with their rating on different sites, number of raters, and review source URL. I stored these URL&#8217;s in CSV (and the raw HTML in Gzipped files).</p>
<h4>Honeypot Features</h4>
<p>One trick to get a lot of web spam URL&#8217;s is to crawl the links from spam e-mails. Spam e-mails often link to spam websites (though sometimes through up to 14 redirects). Another trick is to set up (unpatched) WordPress and Joomla installs to study the trends in webshop hacks (where entire sites are replaced with a functional webshop by spammers). One can also follow the spam comments back to their spam site owners.</p>
<h3>G2) Search the official registers</h3>
<p>Dutch webshops are required to be registered at the Dutch Bureau of Commerce. After that they receive a unique KVK and BTW number. This is often listed on the websites, and can be manually checked (at the cost of 4 eurocents per number). This makes this feature too costly for tens of thousands of sites.</p>
<p>Bankruptcies are publicly published. This may provide features like &#8220;operates_web_shop_while_bankrupt&#8221; which should be very indicative of fishy behaviour.</p>
<p>Manufacturers often publish lists of official retailers and dealers, though all too often in the shape of some beautiful (but terribly inaccessible) JavaScript shopfinder. I am thinking of mailing the brands of often counterfeited products (fashion,handbags,sunglasses) to aquire a machine-readable list of their official retailers.</p>
<h3>G3) Check Pricing</h3>
<p>Another feature that is hard to compute at web scale. For this one needs to build a price watch, so one can spot anomalies like 200$ designer couches. Outside the scope of this classifier for now.</p>
<h3>G4) Check the terms of service</h3>
<p>Natural language processing on the terms of service would be hard, and there is no guarantee that it will be a signal (as scam webshop could easily fake their terms of service). Presence in official registers should already mean that terms of service are in order.</p>
<h3>G5) Spelling errors on the site</h3>
<p>Glaring spelling errors could be caught with a reasonable amount of false positives. A spell-check on the visible text will give the number of errors.</p>
<h3>G6) Contact information on site</h3>
<p>A parser can detect contact information like: Address, phone numbers and e-mails. There is no guarantee that this information is real, and checking legitimacy would be a hard task of cross-referencing multiple data sources.</p>
<p>Phone numbers and e-mails addresses can be checked for membership in scammer blacklists.</p>
<h2>Intelligence Features</h2>
<p>Using blacklists and alert lists one can often to a reasonable degree find even more domains owned by the same scammers. Sometimes a Reverse IP Domain Check unveils a server full of other scam webshops. Or a WHOIS search unveils a known blackhat SEO registrant with multiple unlisted domains. Or exact match searches unveil a network of copy sites. These features are manual labour intensive, though a big scam problem is often caused by a few scammers, making these features possibly valuable.</p>
<p>Users of the model could also provide feedback and report spam and scams. For this I created a special purpose complaint and review list of webshops. I plan to crawl a large list of webshop URL&#8217;s in the future to add to this intelligence: Adherence to Google Webmaster Guidelines, visibility of contact information and more.</p>
<p>This to-do list is currently nearing a 100 Dutch sites/networks selected for further analysis.</p>
<h2>Technical features</h2>
<p>The paper &#8220;<a href="http://cseweb.ucsd.edu/~jtma/papers/url-icml2009.pdf">Identifying suspicious URLs: An Application of Large-Scale Online Learning</a>&#8221; explores multiple online machine learning approaches for detecting malicious web sites (those involved in criminal scams).</p>
<p>These features can work, where blacklists or G1-signals can fail (for example: when these are not up-to-date).</p>
<h3>Lexical URL features</h3>
<ul>
<li>Tokenize the scheme, hostname, path, TLD and parameters (bag of words)</li>
<li>Length of hostname and path</li>
<li>Number of dots in URL (or better number of subdomains)</li>
<li>Number of ?, -, /, = and _ in URL</li>
</ul>
<h3>Host-Based features</h3>
<p>Malicious Web sites may be hosted in less reputable hosting centers, use disreputable registars and often use relatively new domains.</p>
<p>For blacklists the paper mentions SpamAssassin, Botnet blacklists, and Phishtank.</p>
<p>The following features are suggested by the paper:</p>
<ul>
<li>IP in blacklist?</li>
<li>Whois properties, like: date of registration, date of update and expiration, registrant name, registrar name, registrant location, whoisguard in place.</li>
</ul>
<pre>Registrant name: WHOISGUARD PROTECTED

Registrant city: PANAMA</pre>
<p><small>Example of a suspicious WHOIS record from a scam webshop.</small></p>
<ul>
<li>Domain name properties: TTL value for DNS records, &#8220;client&#8221;, IP, or &#8220;server&#8221; in hostname? Is there a PTR record?</li>
<li>Geographic properties: Which continent, country, city does the IP belong to? What is the connection speed?</li>
</ul>
<h3>HTML-Source features</h3>
<ul>
<li>External domain names inside (inline) JavaScript.</li>
<li>Presence of eval() function in JavaScript.</li>
<li>Presence of social links.</li>
<li>Tokenize visible text</li>
<li>Hashed token similarity to a set of already labeled malicious sites.</li>
<li>number of i-frames</li>
<li>full-size i-frame?</li>
<li>LDA on visible text and meta contents</li>
<li>Presence of generators (WordPress), payment methods, shopping carts and plug-ins.</li>
<li>Number of redirects</li>
<li>Cookie set?</li>
</ul>
<h2>What&#8217;s next?</h2>
<p>Sharing the progress on Github. Improving on the crawler. Generate features from samples and create a train set. Build a Vowpal Wabbit model. See the <a href="https://github.com/JohnLangford/vowpal_wabbit/wiki/Malicious-URL-example">Vowpal Wabbit malicious URL example</a>.</p>
<p>Experiment with a URL-to-Crawl-to-features API vs. URL-to-Database-check-to-features API vs. an implementation of the Vowpal Wabbit model in JavaScript, and a large part of the feature generation through client-side JavaScript too.</p>
<p>This series will be on hold until I finish the <a href="https://www.kaggle.com/c/avito-prohibited-content">Avito&#8217;s: the Hunt for Prohibited Content</a> challenge on Kaggle. It should allow me some practical insights in classifying illicit content in advertisements.</p>
<h2>Further reading and notes</h2>
<ul>
<li><a href="http://cseweb.ucsd.edu/~voelker/pubs/eaas-ccs12.pdf">Manufacturing compromise: The emergence of exploit-as-a-feature</a>.</li>
<li><a href="https://www.kaggle.com/c/avito-prohibited-content">Predict which ads contain illicit content at Kaggle</a></li>
<li><a href="http://www.sysnet.org.pk/wiki/images/b/b7/Beyond_Blacklists_ZainabAbaid.pdf">Beyond Blacklists: Learning to Detect Malicious Web Sites from Suspicious URLs</a></li>
</ul>
<h3>Example Crawler output</h3>
<pre>&gt;&gt; url = "http://mlwave.com/human-ensemble-learning/"
&gt;&gt; soup = create_soup(url)	
&gt;&gt; features = create_page_features(soup,url)

&gt;&gt; print json.dumps(features, sort_keys=True, indent=4)

loading from cache
{
    "link": {
        "canonical": "http://mlwave.com/human-ensemble-learning/"
    }, 
    "links_external": [
        "http://arxiv.org/pdf/0911.0460.pdf", 
        "http://beatingthebenchmark.blogspot.com/", 
        "http://blog.kaggle.com/", 
		... 
        "https://twitter.com/mlwave", 
        "https://www.mturk.com/mturk/welcome", 
        "https://www.youtube.com/watch?v=sRktKszFmSk"
    ], 
    "links_internal": [
        "/#content", 
        "/#search-container", 
        "/",
        ...
        "/predicting-repeat-buyers-vowpal-wabbit/", 
        "/winning-2-kaggle-in-class-competitions-on-spam/", 
        "/wp-content/uploads/2014/07/writing-1-distort-11.png"
    ], 
    "links_javascript": [], 
    "meta": {
        "author": "", 
        "contact": "", 
        "copyright": "", 
        "description": "", 
        "generator": "WordPress 3.9.1", 
        "googlebot": "", 
        "keywords": "", 
        "language": "", 
        "robots": ""
    }, 
    "pagetitle": "Human Ensemble Learning | MLWave",
    "root_base": "http://mlwave.com",
    "text_meta": "meta charset utf 8 ... 3 9 1 name generator", 
    "text_visible": "human ensemble learning mlwave ... creative commons 3 0 attribution"
}
</pre>
<p><small>The intro image is from a promotional poster for the 1962 movie &#8220;<a href="http://www.imdb.com/title/tt0052646/">The Brain That Wouldn&#8217;t Die</a>&#8220;</small></p>
]]></content:encoded>
			<wfw:commentRss>http://mlwave.com/detecting-counterfeit-webshops-part-1-feature-engineering/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Reflecting back on one year of Kaggle contests</title>
		<link>http://mlwave.com/reflecting-back-on-one-year-of-kaggle-contests/</link>
		<comments>http://mlwave.com/reflecting-back-on-one-year-of-kaggle-contests/#comments</comments>
		<pubDate>Thu, 31 Jul 2014 15:45:31 +0000</pubDate>
		<dc:creator><![CDATA[mladmin]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://mlwave.com/?p=621</guid>
		<description><![CDATA[It&#8217;s been a year since I joined Kaggle for my first competition. Back then I didn&#8217;t know what an Area Under the Curve was. How did I manage to predict my way to Kaggle Master? Early start Toying with datasets and tools I was already downloading datasets from Kaggle purely for my own entertainment and [&#8230;]]]></description>
				<content:encoded><![CDATA[<p><strong>It&#8217;s been a year since I joined Kaggle for my first competition. Back then I didn&#8217;t know what an Area Under the Curve was. How did I manage to predict my way to Kaggle Master?</strong></p>
<h2>Early start</h2>
<h3>Toying with datasets and tools</h3>
<p>I was already downloading datasets from Kaggle purely for my own entertainment and study before I started competing. Kaggle is one of the few places on the internet where you can get quality datasets in the context of a commercial machine learning problem.</p>
<p><span id="more-621"></span></p>
<p>The dataset for the &#8220;<a href="https://www.kaggle.com/c/amazon-employee-access-challenge/data">Amazon.com &#8211; Employee Access Challenge</a>&#8221; was one of the first datasets that caught my eyes. <a href="https://www.kaggle.com/c/digit-recognizer/forums/t/4219/vowpal-wabbit-benchmark-outperforms-random-forest-and-knn-benchmarks/">This post on using Vowpal Wabbit</a> as a classifier on the MNIST dataset with good result made me interested in studying VW:</p>
<blockquote><p>Or maybe not; such a simple (linear) algorithm really has no right being so good for a problem like this&#8230;</p></blockquote>
<h3>The Kaggle forums</h3>
<p>It was <a href="https://www.kaggle.com/users/66023/miroslaw-horbal">Miroslaw Horbal</a> who really got me hooked on the Kaggle forums with his post: <a href="https://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/4838/python-code-to-achieve-0-90-auc-with-logistic-regression">How to achieve 0.90AUC with Logistic Regression</a>. The thread would amass many replies, all of wonderful sharing nature. I&#8217;d revisit the thread long after the contest had ended, to read up on the cross-validation loop and parameter tuning approaches.</p>
<h2>Competing on Kaggle</h2>
<h3>StumbleUpon Evergreen Classification Challenge</h3>
<p><img class="alignright size-thumbnail wp-image-633" src="http://mlwave.com/wp-content/uploads/2014/07/stumbleupon-150x100.png" alt="Stumbleupon" width="150" height="100" />This was my first contest. After reading &#8220;<a href="http://fastml.com/a-bag-of-words-and-a-nice-little-network/">A bag of words and a nice little neural network</a>&#8221; on FastML I felt confident enough to try out Vowpal Wabbit and TfidfVectorizer. It got me a competative score, but not yet top 25%.</p>
<p>That all changed when <a href="http://www.kaggle.com/users/5309/abhishek">Abhishek Thakur</a> shared his <a href="https://www.kaggle.com/c/stumbleupon/forums/t/5680/beating-the-benchmark-leaderboard-auc-0-878">beat the benchmark post</a> on the forums. Suddenly I had a solid NLP solution in hands, using <a href="http://pandas.pydata.org/">Pandas</a>, <a href="http://www.numpy.org/">Numpy</a> and <a href="http://scikit-learn.org/stable/">SKlearn</a> to put a really good score on the public leaderboard. I really dissected that script, familiarizing myself with every line. I later read that participants in Data Science bootcamps use this benchmark code too when working on this challenge.</p>
<p>After a few weeks I found a way to improve the score by using WordnetLemmatizer from <a href="http://www.nltk.org/">NLTK</a>. I shared this approach in the thread.</p>
<p>I finished top 25%. A few submissions would have scored top 10% had I selected these.</p>
<h4>Lessons learned</h4>
<ul>
<li>The Bag-of-words approach to NLP works</li>
<li>Vowpal Wabbit and Scikit-learn are practical ML libraries</li>
<li>Kaggle trick: Fitting TFIDF on combined train and test set improves score</li>
<li>You can learn a lot from the Kaggle competition forums</li>
<li>Sharing on the Kaggle forums is fun and welcomed</li>
</ul>
<h4>Resources</h4>
<ul>
<li><a href="https://www.kaggle.com/c/stumbleupon/forums/t/6184/what-did-you-use">Post-competition &#8220;what did you use?&#8221; thread</a></li>
<li><a href="http://beatingthebenchmark.blogspot.com/2013/11/stumbleupon-evergreen-classification.html">Beat the benchmark blogspot</a> by Abhishek</li>
</ul>
<h3>Partly Sunny with a Chance of Hashtags</h3>
<p><img class="alignleft size-thumbnail wp-image-634" src="http://mlwave.com/wp-content/uploads/2014/07/crowdflower-150x100.png" alt="crowdflower" width="150" height="100" />This was a fun <a href="https://www.kaggle.com/c/crowdflower-weather-twitter">contest, credit Crowdflower</a>, where the task was to predict weather events from the text in tweets. Using Vowpal Wabbit and csoaa I was able to produce multi-class multi-label predictions. I was never really competitive in this competition.</p>
<p><a href="https://www.kaggle.com/users/68749/tim-dettmers">Tim Dettmers</a>&#8216; post <a href="https://www.kaggle.com/c/crowdflower-weather-twitter/forums/t/6046/how-to-get-started-in-python-sklearn">How to get started in Python/SKlearn</a> did not change that. The &#8220;general outline&#8221; post seemed to give the Kaggle experts a lot of information, but was less useful to me at the time. For example Tim did not specify the algorithm used, yet his approach would annihilate my VW&#8217;s score.</p>
<p>I finished in 86th place.</p>
<h4>Lessons learned</h4>
<ul>
<li>Insights from similar Kaggle competitions stack up.</li>
<li>Even when not producing competitive results one is still learning more about the tools and algorithms.</li>
<li>A Kaggle competition is a game of optimization: every other decent contestant will try out the same algorithms.</li>
</ul>
<h3>Galaxy Zoo &#8211; The Galaxy Challenge</h3>
<p><img class="alignright size-thumbnail wp-image-637" src="http://mlwave.com/wp-content/uploads/2014/07/galaxy-150x100.png" alt="Galaxy Zoo" width="150" height="100" />I participated in this contest to <a href="https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge">classify the morphology of distant galaxies</a>, until the train and test datasets were updated and my submissions were removed. I&#8217;d never done an object recognition task before.</p>
<p>Taking inspiration from the &#8220;central pixels&#8221; Go benchmark posted by admin Joyce and foolhardily using insights from the previous NLP competitions, I tried a bag-of-words approach and <a href="http://radimrehurek.com/gensim/similarities/docsim.html">Gensim&#8217;s Document Similarity</a> class.</p>
<p>The Frankenstein bruteforce cosine similarity search on the thousands of image blocks took multiple days to compute. The final model, public leaderboard rank of around rank 50, took me 4.5 days to generate predictions with (well actually over a week&#8230; when an update forced a restart).</p>
<h4>Lessons learned</h4>
<ul>
<li>I build something akin to what I later learned is a KNNClassifier with distance weighting: calculate the closest image neighbors and weigh their labels according to distance.</li>
<li>Bag-of-words can even work in image classification techniques</li>
<li>I can (hope to) beat Kaggle leaderboard benchmarks in tasks I&#8217;ve never done before.</li>
</ul>
<h3>Facebook Recruiting III &#8211; Keyword Extraction</h3>
<p><img class="alignleft size-thumbnail wp-image-638" src="http://mlwave.com/wp-content/uploads/2014/07/facebook-150x100.png" alt="Facebook" width="150" height="100" />Here the task was to<a href="https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction"> recommend tags for StackOverflow questions</a>. With 30 days left I took up <a href="https://www.kaggle.com/users/3258/william-cukierski">William Cukierski</a>&#8216;s informal challenge on the forums: Create a good model and do training and testing with less than 1GB of RAM and in under 1 hour. In contrast: Some winner(s) used over 80GB of disk swapped memory and 14 hours to build the model using millions of questions. See: <a href="https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/forums/t/6650/share-your-approach">Share your approach?</a></p>
<p>With 200k possible tags to choose from even VW was too slow for this on a single machine. So for this challenge I had to write my own algorithm. The first algorithmic approach was very simple: remove duplicates using hash-tables, predict a tag when that tag is mentioned anywhere in the title:</p>
<pre>How do you install PHP 5.2 on IIS 7?

tags predicted: php iis
</pre>
<p>Using the duplicates in train and test set and this simple tag look-up method I ranked around #20 in 3 minutes. By then, not many had found these duplicates.</p>
<p>After the duplicates became known to all contestants this score dropped significantly against more advanced Bayesian methods.</p>
<p>In the end I managed to score around ~top 33% with a home-made solution that ran in under 1 hour and used 1.6GB of memory. Still pretty proud about that one.</p>
<h4>Lessons learned</h4>
<ul>
<li>One can use batch tf-idf fitting on huge datasets</li>
<li>Count all the things, just don&#8217;t count &#8216;em twice. Keeping just token count dictionaries (and occasional pruning) is a mighty powerful approach when coupled with probabilistic algorithms. Check for duplicates between train and train, test and test and train and test.</li>
<li>Have some fun with the good datasets (generate topics, look at your predictions, invent simple algo&#8217;s to run over it)</li>
<li>Speed, though not often measured in these competitions, is a huge benefit and something to aim for. It allows for faster iterations and scaleable good-enough solutions.</li>
</ul>
<h3>Personalized Web Search Challenge</h3>
<p><img class="alignright size-thumbnail wp-image-639" src="http://mlwave.com/wp-content/uploads/2014/07/yandex-150x100.png" alt="Yandex" width="150" height="100" />This contest was organized by Yandex and featured a massive <a href="https://www.kaggle.com/c/yandex-personalized-web-search-challenge/data">SERP clicklog dataset</a>. The task was to improve the quality of these search results through personalization.</p>
<p>I had a very simple idea: re-order results based on number of clicks by a searcher. If a search for &#8220;widget&#8221; would get 10 clicks on the 3rd result and 1 click on the first result, then re-order based on that.</p>
<p>Execution was a little more difficult, with the custom parse log and huge dataset size. Using a database and querying it I managed to produce a reasonable solution.</p>
<h4>Lessons learned</h4>
<ul>
<li>There are people who do this specific ML task for a living and then you simply have little chance of beating them. When a team from Yandex joined, they blew the competition away.</li>
<li>Using an extremely simple hunch, you have a decent chance of beating people who do general ML tasks for a living, provided you manage to execute.</li>
</ul>
<h2>Getting halfway decent</h2>
<p>By now I was doing most available Kaggle contests, not top 25% material, but still managing to provide halfway decent solutions in the majority of these.</p>
<p>I wrote a <a href="http://mlwave.com/kaggle-connectomics-python-benchmark-code/">Python benchmark for the Connectomics challenge</a>, the <a href="http://mlwave.com/predicting-repeat-buyers-vowpal-wabbit/">Acquire Valued Shoppers Challenge</a>, <a href="http://mlwave.com/predict-visual-stimuli-from-human-brain-activity/">DecMeg2014 &#8211; Decoding the Human Brain</a>, <a href="http://mlwave.com/predicting-click-through-rates-with-online-machine-learning/">Criteo Ad Click Prediction</a>, <a href="https://www.kaggle.com/c/forest-cover-type-prediction/forums/t/8182/first-try-with-random-forests-scikit-learn">Forest Cover Type detection challenge</a> and <a href="http://mlwave.com/movie-review-sentiment-analysis-with-vowpal-wabbit/">Movie Review Sentiment Analysis</a>.</p>
<h3>Lucky wins</h3>
<p><img class="alignleft size-thumbnail wp-image-640" src="http://mlwave.com/wp-content/uploads/2014/07/asus-150x100.png" alt="Asus" width="150" height="100" />I did well in a few <a href="http://mlwave.com/winning-2-kaggle-in-class-competitions-on-spam/">Kaggle Inclass competition</a>s and I got two top 25% positions in forecasting challenges. I was not familiar with forecasting challenges yet so in both PAKDD 2014 and Walmart Recruiting challenge I got lucky with simple hunches, instead of using  the more advanced forecasting algorithms.</p>
<p>For the <a href="https://www.kaggle.com/c/pakdd-cup-2014">PAKDD 2014 Asus</a> challenge the task was to predict the number of repairs each month for computer parts. I noticed when graphing the monthly repairs that this showed a logarithmic decay. So I simply did a math.log() on the repair values and fitted a linear line on this graph. Minor tweaking of slope and elevation gave a top 25% position.</p>
<p>With the <a href="https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting">Walmart Recruiting challenge</a> the task was to predict sales on certain days. You could get a top 25% position by predicting the sales from last year for that day. One could further increase the score by &#8220;leaderboard-validating&#8221;:</p>
<pre>if "storeid" == 103 and "departmentid" == 18 then sales = sales * 1.05
</pre>
<p>vs.</p>
<pre>if "storeid" == 103 and "departmentid" == 18 then sales = sales * 0.95.
</pre>
<p>I quickly grew tired of this, but judging from the number of submissions made by the top contestants, this would have worked.</p>
<h3>Biggest upsets</h3>
<p><img class="alignright size-thumbnail wp-image-641" src="http://mlwave.com/wp-content/uploads/2014/07/mlsp2014-150x100.png" alt="MLSP 2014" width="150" height="100" />I saw a massive drop in Public leaderboard vs. Private leaderboard in the <a href="https://www.kaggle.com/c/mlsp-2014-mri">MLSP Schizophrenia detection challenge</a>. (91AUC to 77AUC). My best submission, and one of the first submissions made, scored 89AUC or top 10. I had picked both my best Public model and a model with 82.5AUC, which I thought to be most robust. I was horribly wrong. With 86 samples in the train set, possibly not unforgivable.</p>
<p>Due to planning issues I did not have enough time to generate the test set with my own improved benchmark code in the <a href="http://connectomics.chalearn.org/">Chalearn Connectomics challenge</a>. This necessitated the need for stricter planning.</p>
<p>I messed up model selection in the <a href="https://www.kaggle.com/c/allstate-purchase-prediction-challenge">Allstate competition</a> when I did too many things in the last minute. Instead of selecting a RF and rule-based model build on many hours of trial and error competing for top 10%, I mistakenly picked a fluke model and did not even get top 25% (even my early benchmark submission would have given a top 25%).</p>
<h3>Kaggle Master</h3>
<p><img class="alignleft size-thumbnail wp-image-642" src="http://mlwave.com/wp-content/uploads/2014/07/master-150x150.png" alt="Kaggle Master" width="150" height="150" />I became <a href="https://www.kaggle.com/users/114978/triskelion">Kaggle Master</a> mostly through ensemble learning, team work, sharing, powerful ML tools and the law of large numbers.</p>
<p>I joined up with <a href="https://www.kaggle.com/users/115173/phil-culliton">Phil Culliton</a> in the <a href="https://www.kaggle.com/c/acquire-valued-shoppers-challenge">Acquire Valued Shopper Challenge</a> and we secured a top 10% using an ensemble of <a href="http://hunch.net/~vw/">Vowpal Wabbit </a>models and a pinch of R&#8217;s <a href="http://cran.r-project.org/web/packages/glmnet/index.html">GLMnet</a>.</p>
<p>With <a href="https://www.kaggle.com/users/163663/shize-su">Shize Su</a>, <a href="https://www.kaggle.com/users/199346/yan-xu">Yan Xu</a>, and <a href="https://www.kaggle.com/users/111640/kazanova">KazAnova</a> we got #5 in the <a href="http://www.kaggle.com/c/kdd-cup-2014-predicting-excitement-at-donors-choose/">KDD-cup 2014</a>. These team members had such good models, that <a href="http://www.kaggle.com/c/kdd-cup-2014-predicting-excitement-at-donors-choose/forums/t/9774/congrats-to-straya-nccu-and-adamaconguli#post50639">my model</a> (which I had given my everything) barely made the cut at all.</p>
<h3>Future</h3>
<p>It would be very nice to win one of the competitions. To consistently place top 10% in these competitions. My teams are ranking well in the currently running <a href="https://www.kaggle.com/c/higgs-boson">Higgs Boson ML challenge</a> and the <a href="https://www.kaggle.com/c/criteo-display-ad-challenge/">Criteo Display Ad challenge</a>. Also with few submissions I am ranking decently in the <a href="https://www.kaggle.com/c/avito-prohibited-content">Avito Detect Prohibited Content </a>challenge.</p>
<p>Though I am a Kaggle Master now, I am nowhere near the skill level of most Kaggle Masters I met on this short wonderful journey. There is plenty of room for improvement, as I haven&#8217;t even touched tools like <a href="http://deeplearning.net/software/pylearn2/">PyLearn2</a>, <a href="http://torch.ch/">Torch</a> or <a href="http://deeplearning.net/software/theano/">Theano</a> yet and VW and Sklearn are adding new and exciting features every release.</p>
<h2>Some Tips</h2>
<p>I think I learned a lot and progressed a lot competing on Kaggle for a year. I am now starting to get a grasp on the more common machine learning problems. If you want to become a Kaggle Master too, next to reading this really insightful article &#8220;<a href="http://blog.kaggle.com/2014/08/01/learning-from-the-best/">Learning from the top Kagglers</a>&#8220;, I can give these tips:</p>
<p><strong>Practice a lot</strong>. Do as many challenges as you can generate a submission for. This buckshot approach will incrementally increase your skills, while hitting the bulls-eye once or twice if you find a good optimization or even something you are good at. You&#8217;ll probably make some mistakes too, like overfitting to the leaderboard. This is ok, provided you learn from this.</p>
<p><strong>Study evaluation metrics</strong>. Try to really understand AUC. What are you optimizing exactly? To produce a good evaluation you need a thorough understanding of the <a href="https://www.kaggle.com/wiki/Metrics">evaluation metric</a>.</p>
<p><strong>Study the problem domain</strong>. Read up on a few business cases and academic papers that mention problems related to the competition problem. What is the state-of-the-art like? You can also get inspiration for feature engineering this way.</p>
<p><strong>Team up</strong>.  For Kaggle Master you need a top 10% and a top 10 finish. Especially those top 10 finishes are hard, when you don&#8217;t know the domain and are competing against other teams of Kaggle Masters. Team up often: to learn how to work as a team on Kaggle competitions, and to meet with others for future co-operations.</p>
<p><strong>Read those forums</strong>. Especially the post-competition threads. Take careful note of solutions and approaches. Revisit these threads when similar competitions arise. For example the KDD-cup 2012 Ad click prediction challenge is mighty similar to the, currently running, Criteo Ad Prediction challenge.</p>
<p><strong>Share on the forums</strong>. It can help with teaming up. It requires you to think about the problem and your solution for it, from many angles and user perspectives. Though sharing too much can hurt your chances of a good score, as people will just take your approach and ensemble it, I still think my abundant sharing on the forums contributed a lot to my Kaggle Master status.</p>
<p><strong>Ensemble learning</strong>. Read up on this and apply it. When done right, it nearly always works. When done expertly, it creates top ~10 submissions with fairly dumb models. Practically I learned about stacking mostly from dissecting code from <a href="https://github.com/emanuele">Emanuele Olivetti</a> and about rank averaging from discussions with <a href="https://www.kaggle.com/users/111640/kazanova">KazAnova</a> (Marios Michailidis). Short, but sweet: The entire <a href="http://scikit-learn.org/stable/modules/ensemble.html">sklearn.ensemble module</a> is golden.</p>
<p><strong>Experiment</strong>. Want to know if you can do Random Forests on bag of words? Find out the point where sparsity, test size and dimensionality start to hamper using RFs? Want to know if RFs are faster to train than logistic regression? Want to find out if you can train RFs on subsampled train chunks and different features and get a reasonable performance?</p>
<p>The answer is always experiment. Try all, discard the bad, keep the good. Then go back to exploring after a year of new knowledge.</p>
<p><strong>Creativity</strong>. You can think inside the box, for example: &#8220;This is binary classification prediction probabilities, so I use logarithmic regression&#8221;. There is black box thinking: &#8220;I put labels and features here, I expect good output there, do it anyway you wanna!&#8221;.</p>
<p>Outside the box creative thinking is more intangible, but can be a huge benefit: you at least have a chance of beating the more proficient competitors using a common predictable approach.</p>
<p><strong>Pick the right tools</strong> and approaches for the job. Before you start tuning individual models or go down a creative rabbit hole you want to find at least a couple of sane algorithmic approaches.</p>
<p><strong>Hyper parameter tuning</strong>. Build a CV loop at least once. Tune every last bit of performance from an SVM. This will hopefully teach you where not to look and hone intuition. There are (extremer) situations where a learning rate is set incredibly small or high. Normally it would be a waste of time to search in these ranges.</p>
<p><strong>Have some fun</strong> along the way. We machine learning lovers live in exciting times. I think we are on the precipice of a new field of technology, with data science and data engineering on the forefront. It&#8217;s a nice budding community online, with places like Kaggle and <a href="http://datatau.com">DataTau</a>. And some very powerful tools are being released for anyone to play with.</p>
<p><small>The intro image is from Wikimedia Commons and depicts the Floriani Tower in Kraków, created by user <a href="http://commons.wikimedia.org/wiki/User:Silar">Silar</a></small></p>
]]></content:encoded>
			<wfw:commentRss>http://mlwave.com/reflecting-back-on-one-year-of-kaggle-contests/feed/</wfw:commentRss>
		<slash:comments>9</slash:comments>
		</item>
		<item>
		<title>Human Ensemble Learning</title>
		<link>http://mlwave.com/human-ensemble-learning/</link>
		<comments>http://mlwave.com/human-ensemble-learning/#comments</comments>
		<pubDate>Sun, 20 Jul 2014 21:58:10 +0000</pubDate>
		<dc:creator><![CDATA[mladmin]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://mlwave.com/?p=452</guid>
		<description><![CDATA[Wisdom of the crowds and ensemble machine learning techniques are similar in principle. Could insights in group learning provide insights in machine learning and vice versa? In this article we will touch upon a variety of more (or less) related concepts and try to build an ensemble view of our own. &#8220;Of all the offspring [&#8230;]]]></description>
				<content:encoded><![CDATA[<p><strong>Wisdom of the crowds and ensemble machine learning techniques are similar in principle. Could insights in group learning provide insights in machine learning and vice versa? In this article we will touch upon a variety of more (or less) related concepts and try to build an ensemble view of our own.</strong></p>
<blockquote><p>&#8220;Of all the offspring of Time, Error is the most ancient, and is so old and familiar an acquaintance, that Truth, when discovered, comes upon most of us like an intruder, and meets the intruder&#8217;s welcome.&#8221; &#8211; <cite>Charles Mackay (1841), Extraordinary Popular Delusions and the Madness of Crowds</cite></p></blockquote>
<h2>Wisdom of the crowds</h2>
<p>The concept of Wisdom of the crowds originated with the book <em>&#8216;The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations&#8217;</em>.<br />
<span id="more-452"></span><br />
This book was published in 2005. The author, James Surowiecki, argues that decisions made by groups are often better than decisions made by a single member of a group.</p>
<h3>Wise crowds</h3>
<p>Surowiecki lists four elements required to form a wise crowd, or a good human ensemble:</p>
<ol>
<li><strong>Diversity of opinion</strong>. Each person should have private information even if it&#8217;s just an eccentric interpretation of the known facts.</li>
<li><strong>Independence</strong>. People&#8217;s opinions aren&#8217;t determined by the opinions of those around them.</li>
<li><strong>Decentralization</strong>. People are able to specialize and draw on local knowledge.</li>
<li><strong>Aggregation</strong>. Some mechanism exists for turning private judgments into a collective decision.</li>
</ol>
<p>Oinas-Kukkonen (2008 &#8211; Network analysis and crowds of people as sources of new organisational knowledge.) follows up with these observations:</p>
<ol>
<li>It is possible to describe how people in a group think as a whole.</li>
<li>In some cases, groups are remarkably intelligent and are often smarter than the smarter people in them.</li>
<li>The three conditions for a group to be intelligent are diversity, independence and decentralization.</li>
<li>The best decisions are a product of disagreement and contest.</li>
<li>Too much communication can make the group as a whole less intelligent.</li>
<li>Information aggregation functionality is needed.</li>
<li>The right information needs to be delivered to the right people in the right place, at the right time, and in the right way.</li>
<li>There is no need to chase the expert.</li>
</ol>
<h3>Foolish crowds</h3>
<p>Sometimes crowds can throw a fit and produce terrible decisions. These extremes are known to cause failures:</p>
<ul>
<li><strong>Extreme homogeneity</strong>. There needs to be diversity in a crowd to ensure enough variance in approach, process and private information.</li>
<li><strong>Extreme centralization.</strong> An hierarchical bureaucracy can close off to wisdom of lower level engineers.</li>
<li><strong>Extreme division.</strong> The United States Intelligence Community failed to prevent the 11 September 2001 attacks partly because information held by one subdivision was not accessible by another. The CIA has created Intellipedia to prevent such failures.</li>
<li><strong>Extreme imitation.</strong> When choices are visible to anyone and made in sequence, an &#8220;information cascade&#8221; can form in which only the first few decision makers gain anything by making a choice, the rest will just follow along and copy.</li>
<li><strong>Extreme emotionality.</strong> Peer pressure, herd instinct and other emotional factors can lead to collective hysteria and bad decisions.</li>
</ul>
<blockquote><p>&#8220;If others would think as hard as I did, then they would get similar results.&#8221; <cite>Isaac Newton</cite></p></blockquote>
<h2>Ensemble Learning</h2>
<p>Ensemble learning or model averaging is used in machine learning to improve performance by grouping individual models.</p>
<h3>Voting</h3>
<p>An example of voting used in ensemble learning is taking the majority vote. Voting is useful for binary yes-no classification problems. Let&#8217;s say you have 5 spam filters. 3 filters predict an email is ham, 2 filters predict an email is spam. The majority vote ensemble would say this email is ham, but with a low certainty.</p>
<p>If all models agree they all vote the same &#8212; the ensemble is sure. If there is an even or very close vote, the ensemble is not so sure. Samples where the voting ensemble is not so sure can be interesting to train further models on, in an effort to increase their discriminatory power.</p>
<h3>Averaging</h3>
<p>Averaging is averaging all predictions. This can be useful for regression problems. Let&#8217;s say you want to predict CPU load in a range of 0-100. You could simply average the prediction output from multiple regression models.</p>
<p>A weighted average is when you give certain models more or less weight. For example when a model with 0.5 weight predicts 0.75 and a model with 1.25 weight predicts 0.6 the ensemble prediction would be adjusted to ( (0.5*0.75 ) + (1.25*0.6) ) / (0.5+1.25) = ~0.64</p>
<p>Instead of averaging the predictions you could also rank these predictions first, and then average the ranks. This approach can help when ensembling predictions from different algorithms, which may have predictions in a different range. Our team used this approach for our <a href="http://www.kaggle.com/c/kdd-cup-2014-predicting-excitement-at-donors-choose">5th place</a> in the <a href="http://www.kdd.org/kdd2014/">KDD-cup 2014</a>.</p>
<h3>Binning</h3>
<p>With binning an individuals model&#8217;s output is put into a bucket. For example with 10 buckets all outputs between 0.1 and 0.2 could be put into the second bin. The number and scale of bins is chosen experimentally.  An ensemble could consist of counting the contents of each buckets and comparing.</p>
<p>Binning was used by competitors in the KDD-cup &#8217;98. When you want probability estimates, you can not reliably average the ranked outputs. This <a href="http://cseweb.ucsd.edu/~elkan/254spring01/jdrishrep.pdf">pdf paper</a> shows their approach with Naive Bayes and SVM. Another use for binning we noticed when Kaggler <a href="http://www.kaggle.com/users/41275/guocong-song">Guocong Song</a> used binning to turn a <a href="http://fastml.com/regression-as-classification/">regression problem into a classification problem</a>.</p>
<h3>Bagging</h3>
<p>Bagging or bootstrap aggregating is a popular approach to ensembling that often increases performance. Many classifiers are trained, each on only subsets of the dataset. Their outputs are then combined through model averaging.</p>
<p><iframe width="474" height="356" src="https://www.youtube.com/embed/Rm6s6gmLTdg?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<p>Breiman (1994 &#8211; <a href="http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf">Bagging Predictors</a>) was one of the first to show theoretically and empirically that aggregating multiple versions of an estimator could increase performance and reduce overfitting/complexity.</p>
<p>A random forest is an ensemble estimator that fits a number of individual decision tree classifiers on various subsets of the train dataset. A random forest uses averaging to improve the predictive accuracy and control over-fitting. For scikit-learn see <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">SKLearn.Ensemble.RandomForest</a> and new in version 0.15: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html">sklearn.ensemble.BaggingClassifier</a>. See also the PhD thesis &#8220;<a href="https://github.com/glouppe/phd-thesis">Understanding Random Forests</a>&#8221; from Kaggler <a href="http://www.kaggle.com/users/1955/gilles-louppe">Gilles Louppe</a>.</p>
<h3>Boosting</h3>
<p>Boosting trains models by sequentially training predictors on samples based on their error rate/certainty. The main idea: Focus new experts on samples that others get wrong. The errors of earlier predictors reveal the samples that are hard to get right, later predictors then focus on getting these samples right.</p>
<p><iframe width="474" height="356" src="https://www.youtube.com/embed/ix6IvwbVpw0?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<p>AdaBoosting is an example of learning from many weaker models to increase the complexity. <a href="https://www.youtube.com/watch?v=sRktKszFmSk">Gradient boosting</a> is another ensemble boosting technique used by algo&#8217;s like Gradient Boosting Machines (GBM).</p>
<h3>Stacking</h3>
<p>Stacking uses a model to predict the better performing models in an ensemble. Every individual model creates predictions for the samples in both the train and the test set. These are then combined into a blended train and test set. Then a predictor (for example logistic regression or GBM) is stacked on top of this and it learns which predictors are closer to the labeled truth.</p>
<p>A variant of stacking, employed by competitors in the Netflix Challenge, is <a href="http://arxiv.org/pdf/0911.0460.pdf">feature-weighted linear stacking</a>. Then the stacked model can learn which predictor is often correct for specific features in the sample. See here another <a href="http://elf-project.sourceforge.net/CombiningPredictionsForAccurateRecommenderSystems.pdf">Netflix competitor paper on blending</a> and the <a href="http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf">winner solution on their ensemble blending approach</a>.</p>
<h3>Good ensemble practices</h3>
<p>Good ensembles are made from models that are:</p>
<ul>
<li><strong>Diverse</strong>. Different algorithms could be trained on different features and different samples. When correlation is low between two models, ensembling these models usually gives better results, opposed to adding already closely correlated models together.</li>
<li><strong>Independent</strong>. Algorithms will overfit (&#8220;memorize&#8221;) when they are trained on some data, and their predictions are used to train on that same data. (stratified) K-fold training should be employed when stacking.</li>
<li><strong>Decentralized</strong>. Algorithms can be trained to focus on a small aspect of a classification or regression problem.</li>
</ul>
<blockquote><p>A gambler, frustrated by persistent horse-racing losses and envious of his friends&#8217; winnings, decides to allow a group of his fellow gamblers to make bets on his behalf.</p>
<p>He decides he will wager a fixed sum of money in every race, but that he will apportion his money among his friends based on how well they are doing.</p>
<p>Certainly, if he knew psychically ahead of time which of his friends would win the most, he would naturally have that friend handle all his wagers.</p>
<p>Lacking such clairvoyance, however, he attempts to allocate each race&#8217;s wager in such a way that his total winnings for the season will be reasonably close to what he would have won had he bet everything with the luckiest of his friends. <cite>The 1996 paper introducing AdaBoost &#8220;<a href="http://cseweb.ucsd.edu/~yfreund/papers/adaboost.pdf">A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting</a>&#8220;.</cite></p></blockquote>
<h2>Ensemble Intelligence</h2>
<h3>The Delphi Method</h3>
<p>The Delphi Method was developed at the beginning of the Cold War to forecast the impact of technology on warfare. In 1944, General Henry H. Arnold ordered the first creation of such a forecasting report for the U.S. Army Air Corps. Multiple approaches were tried and tested, and in 1968 the Delphi Method was created at RAND. It is in use as of today.</p>
<ul>
<li><a href="http://www.rand.org/pubs/papers/P3558.html">Analysis of the Future &#8211; The Delphi Method</a></li>
<li><a href="http://www.rand.org/pubs/research_memoranda/RM5888.html">An Experimental Study of Group Opinion &#8211; The Delphi Method</a></li>
<li><a href="http://www.rand.org/pubs/papers/P1513.html">On the Epistemology of the Inexact Science</a>s</li>
</ul>
<p>In the Delphi Method questionnaires are given for two or multiple rounds. After every round a summery is given where the participants provide reasoning for their judgments. The hope is that the group will converge iteratively to the optimal judgment.</p>
<p>The Delphi method relies on the assumption that group judgments are more valid than individual judgments. And structured group judgments are better than disorganized groups.</p>
<p>Characteristic for the Delphi Method is:</p>
<ul>
<li>The <strong>anonymity of all participants</strong>. This prevents a &#8220;bandwagon effect&#8221; and peer pressure.</li>
<li><strong>Structured information flow</strong>. A panel director makes sure that problems arising from group dynamics are dealt with, controlling the interactions between the participants.</li>
<li><strong>Regular feedback</strong>. Participants comment on their own forecasts, the responses of others and on the progress of the panel as a whole.</li>
<li><strong>Role of the facilitator</strong>. The person coordinating the Delphi method is usually known as a facilitator or Leader. He or she identifies conflicting viewpoints and works to creating a group consensus.</li>
</ul>
<h3>The Good Judgment Project</h3>
<p>The <a href="http://www.goodjudgmentproject.com/">Good Judgment project</a> is an experiment set up by psychologists and intelligence experts. The experiment lets civilians answer questions like &#8220;Will North Korea launch a new multistage missile before August 10, 2014?&#8221;.</p>
<p>When certain civilians turn out to be correct more often than their (expert) peers, their answers are given increasingly more weight. After a while, one can identify a smaller team of top civilians that may collectively (or even individually) outperform CIA analysts.</p>
<blockquote><p>&#8220;I&#8217;m just a pharmacist. Nobody cares about me, nobody knows my name, I don&#8217;t have a professional reputation at stake. And it&#8217;s this anonymity which actually gives me freedom to make true forecasts.&#8221; <cite>Elaine Rich on <a href="http://www.npr.org/blogs/parallels/2014/04/02/297839429/-so-you-think-youre-smarter-than-a-cia-agent">being smarter than a CIA analyst</a></cite></p></blockquote>
<h2>Decision theory</h2>
<p>Decision theory in economics, psychology, philosophy, mathematics, and statistics is concerned with identifying the values, uncertainties and other issues relevant in a given decision, its rationality, and the resulting optimal decision.</p>
<h3>Garbage Can Model of Organizational Choice</h3>
<p>In 1972 a novel approach to decision theory was proposed by <a href="http://www.unc.edu/~fbaum/teaching/articles/Cohen_March_Olsen_1972.pdf">March, Cohen &amp; Olsen</a>. Instead of the static and rigid problem-solver-solution path, they wrote a paper that disconnected problems from solutions and decision makers.</p>
<p>Giant institutions often struggle to act in a timely or effective manner. Take for instance a university. A problem may arise: students complain about lack of sporting facilities. After a lot of actions a final decision is made: build a basketball court on campus.</p>
<p>A year later after budgeting, the construction begins and new problems arise: recalculations show it&#8217;s not economically viable, unsafe work traffic, noise complaints. The new decision makers in office are now faced with an older problem and an unrewarding solution.</p>
<p>Meanwhile the students have already solved this problem by joining the nearby fitness gym (and really don&#8217;t even like basketball anymore). What does it take for an organization to make a rational choice: To arrange a student discount with the fitness facility and halt construction?</p>
<blockquote><p>This document describes a proposed design for a globally distributed artificial general intelligence (AGI) for the purpose of automating the world economy. The estimated value is on the order of US $1 quadrillion. The cost of a solution would be of the same order if we assume a million-fold decrease in the costs of computation, memory, and bandwidth, solutions to the natural language, speech, and vision problems, and an environment of pervasive public surveillance.</p>
<p>The high cost implies decentralized ownership and a funding model that rewards intelligence and usefulness in a hostile environment where information has negative value and owners compete for attention, reputation, and resources.<cite><a href="http://mattmahoney.net/agi2.html">A Proposed Design for Distributed Artificial General Intelligence</a></cite></p></blockquote>
<h2>Context Mixing</h2>
<p>Marcus Hutter has put up a prize of <a href="http://www.hutter1.net/prize/">50.000$ for the best compression algorithm</a>. Some AI researchers, like <a href="http://mattmahoney.net/dc/dce.html">Matt Mahoney</a>,  view compression as related to understanding. If you can <a href="http://www.kaggle.com/c/billion-word-imputation">impute words in a sentence</a>, you understand that sentence. If you can impute words in a sentence you won&#8217;t have to store these words. Optimal compression of text will require a deep understanding of semantics, syntax, style, culture etc. Better compressors bring us closer to AI.</p>
<p>One of the best compression libraries is the PAQ series. The code trades computational cost and memory vs. a high compression ratio. The PAQ series uses what is known as <a href="http://en.wikipedia.org/wiki/Context_mixing">context mixing</a>: Two or more statistical models are combined (using averaging/bagging, stacking with a Random Forest, Bayesian updates, <a href="http://cs.fit.edu/~mmahoney/compression/cs200516.pdf">adaptive weighing</a>, or, in more recent versions neural networks) to form better predictions.</p>
<blockquote><p>&#8220;All data are created equal but some data are more alike than others&#8221; <cite><a href="http://homepages.cwi.nl/~paulv/papers/cluster.pdf">Clustering by compression</a></cite></p></blockquote>
<h2>Robots on Mars</h2>
<p>In 1990 Luc Steels wrote the paper &#8216;<em>Cooperation between distributed agents through self-organisation</em>&#8216;. In the paper he presents a case study for resource gathering on a distant planet using autonomous agents.</p>
<p>For mobile robots to perform well as a group on a distant planet the following criteria are relevant:</p>
<ul>
<li><strong>Robustness</strong>: The system should be able to recover when a certain action is not correctly executed. When a sample is not picked up, while the command to pick it up was given, this should not lead to further malfunction.</li>
<li><strong>Graceful performance degradation</strong>: Loss of one robot should not be fatal for the group.</li>
<li><strong>Flexibility</strong>: When conditions in the environment change, this should not cause an incapability to function. When resources become scarce in one area, the group system should adapt.</li>
<li><strong>Hardware, Communication and Cognitive economy</strong>: This refers to the trade-offs between increased complexity and the increased energy needed to keep a system running. Less complexity is better. We see this in human communication too: text strings can usually be compressed by a lot, they are not complex. Using short and simple words for often used expressions saves us energy.</li>
<li><strong>Predictability</strong>: This refers to the amount of regularity that has to be present in the environment for the total system to keep functioning. An efficient group is able to cope with unpredictable situations.</li>
<li><strong>Prior knowledge</strong>: The amount of information that has to be known for the system to operate. For example, does a map of the terrain need to generated first? Requiring less prior knowledge is better.</li>
</ul>
<p><iframe width="474" height="267" src="http://www.youtube.com/embed/93LwvuxDbfU?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<p>Luc Steels envisioned agents operating in a group, much like ants do when gathering resources: Robots could leave little pellets of radioactive material behind, to create slowly decaying trails for other robots to follow.</p>
<blockquote><p>We find that whole communities suddenly fix their minds upon one object, and go mad in its pursuit; that millions of people become simultaneously impressed with one delusion, and run after it, till their attention is caught by some new folly more captivating than the first.<cite>Charles Mackay (1841), Extraordinary Popular Delusions and the Madness of Crowds</cite></p></blockquote>
<h2>Multi-Armed Bandits</h2>
<p>The problem of gathering resources in the most efficient way is an old (and difficult) one. From 5 slot machines with different payouts, how do you find out the best slot machine to play?</p>
<p>One technique is an epsilon-greedy Multi-Armed Bandit (MAB). And epsilon-greedy MAB starts out by exploring: trying many different slot machines. The exploiting rate is still low: it will not try the same slot machine over and over. After it has played some more the exploiting rate goes up. In the end it will play the slot machine with the highest expected payout.</p>
<p>You can set the MAB to be flexible to environment changes. Simply add a decay factor to the memory of the payouts. And don&#8217;t be too greedy: always explore a little to find good alternatives.</p>
<blockquote><p>[The bandit problem] was formulated during the [second world] war, and efforts to solve it so sapped the energies and minds of Allied analysts that the suggestion was made that the problem be dropped over Germany, as the ultimate instrument of intellectual sabotage.<cite>Peter Whittle</cite></p></blockquote>
<h2>Swarm intelligence</h2>
<p>Swarm intelligence is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept was introduced by Beni &amp; Wang in a 1989 paper on <a href="http://link.springer.com/chapter/10.1007%2F978-3-642-58069-7_38">cellular robotic systems</a>.</p>
<p><iframe width="474" height="267" src="https://www.youtube.com/embed/kjYFN4ElzzE?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<p><a href="http://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms">Ant colony optimization algorithms</a> can solve the travelling salesman problem (finding the shortest paths between a number of resources). <a href="http://en.wikipedia.org/wiki/Particle_swarm_optimization">Particle Swarm Optimization</a> uses insights from social behavior to create very efficient algorithms, more recently improved with <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=1614658&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1614658">local search</a>.</p>
<blockquote><p>&#8220;Ants are the history of social organization and the future of computers.&#8221; <cite>Kevin Kelly</cite></p></blockquote>
<h2>Crowdsourcing</h2>
<h3>Kaggle</h3>
<p><a href="http://kaggle.com">Kaggle</a> connects data science problems with data scientists. When you ensemble the models made by individual team members this team model is often better than any individual member. After a contest is over, incorporating the shared approaches from other teams often increases this score even more. Human decisions on the team model, such as which submission to select for final ranking, or what the weight of individual models should be, is often put to a vote.</p>
<h3>Amazon Mechanical Turk</h3>
<p>With <a href="https://www.mturk.com/mturk/welcome">Amazon&#8217;s Mechanical Turk</a> (artificial artificial intelligence) one can automate crowdsourcing all sorts of tasks. Even tasks that may seem impossible for individuals to get right. Here is an experiment using Mechanical Turk to transcribe blurry text.</p>
<p><a href="http://mlwave.com/wp-content/uploads/2014/07/writing-1-distort-11.png"><img class="alignnone size-full wp-image-555" src="http://mlwave.com/wp-content/uploads/2014/07/writing-1-distort-11.png" alt="Distorted writing" width="500" height="400" /></a></p>
<p><i>Can you read this text? <a href="http://groups.csail.mit.edu/uid/deneme/?p=329">The ensemble of Mechanical Turks</a> made just 1 mistake.</i></p>
<blockquote><p>&#8220;Winners are generally not domain experts but machine learning experts, we often have people working on 2-3 competitions at the same time&#8221; <cite><a href="https://twitter.com/jeremyphoward">Howard</a> on Kaggle</cite></p></blockquote>
<h2>China Brain</h2>
<div id="attachment_591" style="width: 308px" class="wp-caption alignright"><a href="http://mlwave.com/wp-content/uploads/2014/07/thought-form.jpg"><img class="size-medium wp-image-591" src="http://mlwave.com/wp-content/uploads/2014/07/thought-form-298x300.jpg" alt="Thought form" width="298" height="300" /></a><p class="wp-caption-text">Artist Annie Besant paints the thought form &#8220;Music of Gounod&#8221;</p></div>
<p>The &#8220;China Brain&#8221; is a thought experiment (philosophy of mind) to try to ridicule the thought that non-biological organisms can have feelings and cognition. If you give the entire nation of China two-way radio&#8217;s and make them relay signals, just like neurons would do, would this form a consciousness or mental state?</p>
<p>In <a href="http://www.uoguelph.ca/~abailey/Resources/Dennett's%20'Consciousness%20Explained'.pdf">Consciousness Explained</a> Dennett argues that the China Brain would indeed have mental states. He and other functionalists do not agree that biological neurons are the only way to create an information processing intelligence.</p>
<p>In the <a href="http://en.wikipedia.org/wiki/Multiple_drafts_model">multiple drafts model</a> our consciousness is distributed and many parts contribute together in parallel. The individual consciousness &#8212; a centralized theater where consciousness resides is an illusion. Our consciousness could be a group effort already, where some parts (&#8220;observing light and dark&#8221;) are faster (less complex, less energy consuming) to execute and parse, than others (&#8220;abstract thought and self-realization&#8221;).</p>
<p>A China Brain could be viewed as a supermeme, a discarnate information processing being (A group mind, <a href="http://en.wikipedia.org/wiki/Memeplexes">memeplex</a>, or <a href="http://en.wikipedia.org/wiki/Egregore">egregore</a>). Where the  judgment to go to war as a country is not made by a few individuals at the top, and collective group judgments outperform the judgments of individual members.</p>
<p><iframe width="474" height="267" src="https://www.youtube.com/embed/KzGjEkp772s?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<blockquote><p>&#8220;There is a species of primate in South America more gregarious than most other mammals, with a curious behavior.The members of this species often gather in groups, large and small, and in the course of their mutual chattering , under a wide variety of circumstances, they are induced to engage in bouts of involuntary, convulsive respiration, a sort of loud, helpless, mutually reinforcing group panting that sometimes is so severe as to incapacitate them.</p>
<p>Far from being aversive, however, these attacks seem to be sought out by most members of the species, some of whom even appear to be addicted to them.</p>
<p>&#8230;the species is Homo sapiens, and the behavior is laughter.&#8221; <cite>Daniel Dennett &#8211; Consciousness Explained</cite></p></blockquote>
<p><small>The intro image came from the Microsoft Research paper: <a href="http://research.microsoft.com/pubs/176525/Cross_Cutrell_Thies-UIST2012-Low-cost%20Audience%20Polling.pdf">Low-Cost Audiance Polling using Computer Vision</a>.</small></p>
]]></content:encoded>
			<wfw:commentRss>http://mlwave.com/human-ensemble-learning/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Predicting CTR with online machine learning</title>
		<link>http://mlwave.com/predicting-click-through-rates-with-online-machine-learning/</link>
		<comments>http://mlwave.com/predicting-click-through-rates-with-online-machine-learning/#comments</comments>
		<pubDate>Wed, 25 Jun 2014 22:43:32 +0000</pubDate>
		<dc:creator><![CDATA[mladmin]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://mlwave.com/?p=470</guid>
		<description><![CDATA[Good clicklog datasets are hard to come by. Luckily CriteoLabs released a week&#8217;s worth of data &#8212; a whopping ~11GB! &#8212; for a new Kaggle contest. The task is to predict the click-through-rate for ads. We will use online machine learning with Vowpal Wabbit to beat the logistic regression benchmark and get a nr. 1 position on [&#8230;]]]></description>
				<content:encoded><![CDATA[<p><strong>Good clicklog datasets are hard to come by. Luckily CriteoLabs released a week&#8217;s worth of data &#8212; a whopping ~11GB! &#8212; for a new Kaggle contest. The task is to predict the click-through-rate for ads. We will use online machine learning with Vowpal Wabbit to beat the logistic regression benchmark and get a nr. 1 position on the leaderboard.</strong></p>
<h3>Updates</h3>
<p><i><br />
<a href="https://github.com/JohnLangford/vowpal_wabbit/tree/master/demo/advertising">Demo with data</a> from this contest added to Vowpal Wabbit. Now that this contest is over: <a href="http://labs.criteo.com/downloads/2014-conversion-logs-dataset/ ">Go here</a> if you want to download the dataset freely made available by Criteo.</p>
<p>The Winning team also won the following <a href="http://www.kaggle.com/c/avazu-ctr-prediction/">Avazu CTR prediction</a> challenge and released <a href="http://www.csie.ntu.edu.tw/~r01922136/libffm/">Field-Aware Factorization Machines</a>.</p>
<p><a href="https://www.kaggle.com/users/78307/guestwalk">Winning team</a> used a mixture of Factorization Machines and GBRT. <a href="https://github.com/guestwalk/kaggle-2014-criteo">Code here</a>. </p>
<p>Highest scoring team using Vowpal Wabbit was <a href="https://www.kaggle.com/users/41275/guocong-song">Guocong Song</a> for 3rd place. <a href="https://github.com/songgc/display-advertising-challenge">Method and code here</a>. In short: Multiple models, polynomial learning and featuremasks.</p>
<p>Highest scoring team using this benchmark (and cubic features) was <a href="https://www.kaggle.com/users/73703/silogram">Silogram</a> for 14th place.</p>
<p><strong>Our team got 29th place</strong> out of 718 competing teams.</p>
<p><a href="https://www.kaggle.com/users/185835/tinrtgu">tinrtgu</a> posted a <a href="https://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/10322/beat-the-benchmark-with-less-then-200mb-of-memory/53674">very cool benchmark</a> on the forums that uses only standard Python libraries and under 200MB of memory. Now is your chance to play around with online learning, the hash trick, adaptive learning and logistic loss and get a score of ~0.46902 on the public leaderboard.</p>
<p>FastML wrote <a href="http://fastml.com/vowpal-wabbit-eats-big-data-from-the-criteo-competition-for-breakfast/">a blog</a> about this competition with some tips to improve this benchmark.<br />
</i></p>
<p>The <a href="http://www.kaggle.com/c/criteo-display-ad-challenge">competition of optimizing online advertisements</a> with machine learning is like strawberries with chocolate and vanilla: You have large amounts of data, an almost endless variety of features to engineer, and profitable patterns waiting to be discovered.</p>
<p><span id="more-470"></span></p>
<p>Identifying (and serving) those ads that have a higher probability of a click, translates into more profit and a higher quality:</p>
<ul>
<li>More profit, because in most online advertisement networks, advertisers are only charged when a user clicks on their ads.</li>
<li>A higher quality, because higher Click-Through-Rates (CTR) implies more relevant advertising (giving a user what he/she wants).</li>
</ul>
<h2>Behavioral Retargeting</h2>
<p>Behavorial retargeting is a form of online advertising where the advertisements are targeted according  to previous user behavior, often in the case where a visit did not result in a sale or conversion.</p>
<h3>ROI</h3>
<p>Like Kagglers may try to optimize AUC (Area under Curve) , online advertisers generally aim to optimize Return on Investment (ROI).</p>
<pre>Return on Investment = Net profit / investment</pre>
<p>Optimizing this formula may be done by:</p>
<ul>
<li>keeping the investment low, for example by looking for cheap, but effective marketing opportunities.</li>
<li>increasing net profits, for example by optimizing the conversion ratio (the amount of web users that turn into paying customers).</li>
</ul>
<p>Behavioral retargeting can increase ROI through:</p>
<ul>
<li>Lowering investment. The networks on which to serve the retargeted ads are usually cheaper than the market average.</li>
<li>Increasing profit. Shopping cart abandonment is a real problem for online retailers. These customers were lost in the latest stages of the conversion chain. Retargeting with products catered to the user wins a certain percentage of them back.</li>
</ul>
<p><img class="alignright size-full wp-image-512" alt="criteo" src="http://mlwave.com/wp-content/uploads/2014/06/criteo1.png" width="200" height="158" /></p>
<p><span style="font-size: 24px; font-weight: bold; line-height: 1;">Criteo</span></p>
<p><a style="line-height: 1.5;" href="http://www.criteo.com/">Criteo</a><span style="line-height: 1.5;"> is a retargeting company working together with online retailers to serve online advertisements.</span></p>
<h3>Prediction engine</h3>
<p>One of the core products of Criteo is a prediction engine. The engine is capable of predicting effective personalized advertisements on a web scale: real-time optimization of ad campaigns, click-through rates and conversion.</p>
<p>For this prediction engine they are always looking for ways to engineer new informative features.</p>
<h3>Behavioral features</h3>
<p>A behavioral numerical feature may be the count of previous purchases. A behavioral categorical feature may be the product ID that was added to a shopping cart, but not purchased.</p>
<p>The engineering blog also shows how Criteo is creating web graphs (without using web crawlers) to engineer new features.</p>
<p><a href="http://mlwave.com/wp-content/uploads/2014/06/graphe_web1.png"><img class="alignnone size-full wp-image-477" alt="Criteo Graph Web" src="http://mlwave.com/wp-content/uploads/2014/06/graphe_web1.png" width="1024" height="768" /></a></p>
<p>&nbsp;</p>
<p><em>The web as seen by Criteo [from the <a href="http://engineering.criteolabs.com/">Criteo Engineering blog</a>]</em></p>
<h2>Show us the data!</h2>
<p>For this contest Criteo&#8217;s R&amp;D division, <a href="http://labs.criteo.com/">CriteoLabs</a>, has released a week&#8217;s worth of click data. CriteoLabs is one of the biggest European R&amp;D divisions with a focus on predictive advertising.</p>
<p>The train set has 45.840.616 labelled ads (click-through &#8220;1&#8243; or not &#8220;0&#8243;). The test set has 6.042.135 unlabelled ads. Both sets are sampled to have a near even distribution of clicks and non-clicks.</p>
<p>We have 13 columns of integer features (mostly count features), and 26 columns with hashed categorical features.</p>
<p>Though the exact nature of the features is unknown to us, <a href="http://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/9568/what-are-the-features/49738#post49738">according to a competition admin</a> (<a href="http://olivier.chapelle.cc/">Olivier Chapelle</a>), they fall in the following categories:</p>
<ul>
<li>Publisher features, such as the domain of the url where the ad was displayed;</li>
<li>Advertiser features (advertiser id, type of products,&#8230;)</li>
<li>User features, for instance browser type;</li>
<li>Interaction of the user with the advertiser, such as the number of the times the user visited the advertiser website.</li>
</ul>
<p>Our task now is to create a model that will predict the probability of a click.</p>
<h2>Online Machine Learning with VW</h2>
<p>I think that sticking with your favorite machine learning tool or algorithm for all classification and regression problems, is like picking a chess opening and only playing that against all opponents. You can get lucky and pick a solid opening that will do well most of the times, or you are stuck with a quirky, outdated, or peculiar opening.</p>
<p>Vowpal Wabbit is how Elmer Fudd would pronounce Vorpal Rabbit. I picture the <a href="https://www.youtube.com/watch?v=XcxKIJTb3Hg">Killer Rabbit of Caerbannog</a> brandishing a +5 <a href="http://en.wikipedia.org/wiki/Magic_item_(Dungeons_%26_Dragons)#Examples">vorpal sword</a>. It would give any player an unfair advantage if they found it early on in the game.</p>
<blockquote><p>One, two! One, two! And through and through. The vorpal blade went snicker-snack!He left it dead, and with its head he went galumphing back. &#8211; Jabberwocky</p></blockquote>
<p>If you follow my <a href="http://mlwave.com">blogs,</a> you&#8217;ll know that I&#8217;ve tried Vowpal Wabbit on many Kaggle competitions. It won&#8217;t perform the best on all competitions, though it will perform in most (Multiclass Classification, Regression, online LDA, Matrix Factorization, Structured Prediction, Neural network reduction, Feature interactions), and is a robust addition to many ensembles.</p>
<p>This competition is made for fast online machine learning (with <a href="http://hunch.net/~vw/">Vowpal Wabbit</a> or <a href="https://code.google.com/p/sofia-ml/">Sofia-ML</a>). Models may quickly become outdated, so will probably constantly need to be retrained (we need fast convergence). The collected click data is huge (often far larger than fits into memory) or unbounded (you may constantly collect new categorical feature values).</p>
<p>For a deeper understanding of large scale online machine learning watch this fine <a href="http://techtalks.tv/talks/online-linear-learning-part-1/57924/">video tutorial with John Langford</a>. I can also suggest this relevant seminal paper &#8220;<a href="http://arxiv.org/pdf/1110.4198.pdf">A Reliable Effective Terascale Linear Learning System</a>&#8220;, where one of the authors is a competition admin.</p>
<h3>Picking a loss function</h3>
<p>Online machine learning with VW learns from samples one at a time. When your model is trained it iterates through the train dataset and optimizes a loss function.</p>
<p>Vowpal Wabbit has five <a href="https://github.com/JohnLangford/vowpal_wabbit/wiki/Loss-functions">loss functions</a>:</p>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Least_squares">Squared loss</a>. Useful for regression problems, when minimizing expectation. For example: Expected return on a stock.</li>
<li>Classic loss. Vanilla squared loss (without the <a href="http://uai.sis.pitt.edu/papers/11/p392-karampatziakis.pdf">importance weight aware update</a>).</li>
<li><a href="http://en.wikipedia.org/wiki/Quantile_regression">Quantile loss</a>. Useful for regression problems, for example: predicting house pricing.</li>
<li><a href="http://en.wikipedia.org/wiki/Hinge_loss">Hinge loss</a>. Useful for classification problems, minimizing the yes/no question (closest 0-1 approximation). For example: Keyword_tag or not.</li>
<li><a href="https://www.kaggle.com/wiki/LogarithmicLoss">Log loss</a>. Useful for classification problems, minimizer = probability, for example: Probability of click on ad.</li>
</ul>
<p>Without seeing <a href="http://www.kaggle.com/c/criteo-display-ad-challenge/details/evaluation">the evaluation page of the competition</a> our hunch should be to pick logaritmic loss. That the competition metric really is logaritmic loss means we gain a massive amount of information even by training a model with Vowpal Wabbit: With its holdout functionality one-in-ten samples will be used to calculate and report on the average loss.</p>
<p>Vowpal Wabbit is fast with its compiled C++ code, but also because it employs the <a href="http://en.wikipedia.org/wiki/Feature_hashing#Feature_vectorization_using_the_hashing_trick">hashing trick</a>. Our feature space is now fixed to a number of bits. We do not need a lookup table in memory to convert text or categorical values into numerical features.</p>
<h2>Munging</h2>
<p>We are going to munge the CSV train and test set to Vowpal Wabbit files (VW files). For this I wrote a Python script csv_to_vw.py.</p>
<p>There are no features to engineer, so we will just put the provided numerical and categorical features neatly into their own namespaces.</p>
<p>A single line from the resulting VW train set (~12GB) now looks like:</p>
<pre>-1 '10000000 |i I9:181 I8:2 I1:1 I3:5 I2:1 I5:1382 I4:0 I7:15 I6:4 I11:2 I10:1 I13:2 |c 21ddcdc9 f54016b9 2824a5f6 37c9c164 b2cb9c98 a8cd5504 e5ba7672 891b62e7 8ba8b39a 1adce6ef a73ee510 1f89b562 fb936136 80e26c9b 68fd1e64 de7995b8 7e0ccccf 25c83c98 7b4723c4 3a171ecb b1252a9d 07b5194c 9727dd16 c5c50484 e8b83407</pre>
<p>Where <code>-1</code> means &#8220;no click&#8221;. <code>'10000000</code> is the ID. <code>|i</code> is the namespace with the numerical features and <code>|c</code> contains the categorical features.</p>
<p>The test set should look similar.</p>
<h2>Training</h2>
<p>We then train the model with the following command:</p>
<pre>./vw -d click.train.vw -f click.model.vw --loss_function logistic</pre>
<p>Where <code>-d click.train.vw</code> says to use our train dataset. <code>-f click.model.vw</code> saves the model. <code>--loss_function logistic</code> is setting our loss function. This runs a single pass over the train dataset. Vowpal Wabbit 7.6.1 reports back an average loss of 0.473238. Certainly not the best possible with VW, but a promising start.</p>
<h3>Testing and submitting</h3>
<p>We are now ready to predict CTR on ~6 million advertisements from the test set. In Vowpal Wabbit we can do this with:</p>
<pre>./vw -d click.test.vw -t -i click.model.vw -p click.preds.txt</pre>
<p>Where <code>-t</code> says to test only, <code>-i click.model.vw</code> specifies the input model and <code>-p</code> saves our predictions. (<i>Edit: Thanks to Anuj for spotting that I forgot to specify the model when testing, code above updated.</i>)</p>
<p>After running the above command we should have our predictions in a text file (~100MB). We now want to turn these into probabilities (using a <a href="http://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a>) and submit them to Kaggle in CSV format.</p>
<p>You could use the provided script vw_to_kaggle.py to do this. The submission scores 0.48059 on public leaderboard which is good enough to beat the benchmark and at the time of writing will net you a second place.</p>
<h2>Code</h2>
<p>All code is available on the <a href="https://github.com/MLWave/kaggle-criteo">MLWave Github</a> account. Improvements and feedback always welcome.</p>
<h2>Conclusion</h2>
<p>The <a href="http://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/9577/what-is-the-baseline/49739#post49739">process for creating baseline benchmark</a> was:</p>
<ul>
<li>All the features were encoded as sparse binary (standard &#8220;one hot&#8221; encoding);</li>
<li>To reduce the dimensionality, the features that took more than 10,000 different values on the training set were discarded;</li>
<li>The learning algorithm was linear logistic regression optimized with l-bfgs;</li>
<li>The regularization coefficient was set to 1.</li>
</ul>
<p>We beat the logistic regression benchmark with our first submission. Vowpal Wabbit truly is an industry-ready tool for machine learning on large and high dimensional datasets.</p>
<p>Now the fun part starts: to optimize the parameters (BTW: Vowpal Wabbit has a utility script for this). Aim for the lowest average logistic loss on your own laptop and produce real business value.</p>
<p>Also as the models increase in complexity, so do their training times. For now we were able to keep the munging, training and predicting under 1 hour.</p>
<h2>Acknowledgements</h2>
<p>I&#8217;d like to thank Kaggle for hosting the competition and <a href="http://www.criteo.com/">Criteo</a> for sharing this amazing dataset. <a href="http://fastml.com/blog/categories/vw/">FastML for introducing Vowpal Wabbit</a> to my game. And of course all the authors of Vowpal Wabbit, by name its principal developer: <a href="http://research.microsoft.com/en-us/people/jcl/">John Langford</a>.</p>
<p>In particular I&#8217;d like to thank the <a href="http://www.kaggle.com/users">Kaggle competitors</a>. When not providing tips on the forums, you are after my leaderboard scores. I know I will probably not be #1 for much longer and I predict that it will only make me work harder, better, faster.</p>
]]></content:encoded>
			<wfw:commentRss>http://mlwave.com/predicting-click-through-rates-with-online-machine-learning/feed/</wfw:commentRss>
		<slash:comments>24</slash:comments>
		</item>
		<item>
		<title>Winning 2 Kaggle in Class Competitions on Spam</title>
		<link>http://mlwave.com/winning-2-kaggle-in-class-competitions-on-spam/</link>
		<comments>http://mlwave.com/winning-2-kaggle-in-class-competitions-on-spam/#comments</comments>
		<pubDate>Mon, 02 Jun 2014 13:36:57 +0000</pubDate>
		<dc:creator><![CDATA[mladmin]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://mlwave.com/?p=435</guid>
		<description><![CDATA[Kaggle hosts certain in Class contests that are free to join for everyone. The problems can be simpler than the main competition problems, so this offers a lot of opportunity to experiment and learn. I&#8217;ll walk you through two competitions that dealt with spam, and tell you how I won them. Fascination with spam I [&#8230;]]]></description>
				<content:encoded><![CDATA[<p><strong>Kaggle hosts certain in Class contests that are free to join for everyone. The problems can be simpler than the main competition problems, so this offers a lot of opportunity to experiment and learn. I&#8217;ll walk you through two competitions that dealt with spam, and tell you how I won them.<br />
</strong></p>
<h2>Fascination with spam</h2>
<p>I have an unnatural fascination with (web) spam. I am trying to familiarize myself with the spammer mindset and their signatures. At times I actually enjoy reading my spam folder.</p>
<p><span id="more-435"></span></p>
<p>I purposely did not install a comment spam filter on this blog. Instead I manually remove 1000s of spam comments, labeling them in the process. Now if only I would get some more ham comments, then I can build a comment spam filter based on actual domain expertise.</p>
<h2>ADCG SS14 Challenge 02 &#8211; Spam Mails Detection</h2>
<p><img class="alignright size-full wp-image-446" alt="Anomaly Detection Challenges" src="http://mlwave.com/wp-content/uploads/2014/06/anomaly-detection-challenges.png" width="213" height="100" />In <a href="http://inclass.kaggle.com/c/adcg-ss14-challenge-02-spam-mails-detection">this anomaly detection contest</a> you are tasked to find out spam in an email corpus. The contest is part of a <a href="http://ml.sec.in.tum.de/adcg/">Master program</a> at the <a href="http://www.tum.de/">Technical University of München</a>.</p>
<p>We are to predict 1829 mails into spam or ham (The test set). We have 2502 labeled mails to do this (The train set). We have both the email headers and the email contents. The format for the emails is .eml. The evaluation metric is classification accuracy.</p>
<h3>Redefining the problem space</h3>
<p>I have much respect for my fellow contestants. To receive course credit they had to design and implement their own algorithms. So they had to get their hands dirty writing Bayesian spam filters, while I could make use of algorithm libraries.</p>
<p>I did try to create my own algorithms at first: One rule-based and one based on the seminal paper by Paul Graham &#8220;<a href="http://www.paulgraham.com/spam.html">A plan for spam</a>&#8220;, but later on I redefined the problem: Just write the best spam filter you possibly can, don&#8217;t reinvent the wheel, but make use of open source software and algorithm libraries.</p>
<h3>Vowpal Wabbit as an online spam filter</h3>
<p><a href="http://hunch.net/~vw/">Vowpal Wabbit</a> has been used as a spam filter for Yahoo mail. (Academic) research on its performance can be found in:</p>
<ul>
<li><a href="http://www.cse.wustl.edu/~kilian/papers/ceas2009-paper-11.pdf ">Collaborative Email-Spam Filtering with the Hashing-Trick</a></li>
<li><a href="http://jmlr.org/papers/volume10/langford09a/langford09a.pdf">Sparse Online Learning via Truncated Gradient</a></li>
<li><a href="http://arxiv.org/pdf/0902.2206.pdf">Feature Hashing for Large Scale Multitask Learning</a></li>
<li><a href="http://www.slideshare.net/jakehofman/technical-tricks-of-vowpal-wabbit">Technical Tricks of Vowpal Wabbit</a></li>
</ul>
<p>Vowpal Wabbit can &#8220;eat&#8221; raw text for its features. You just have to clean the text a little:</p>
<pre>1 'email_spam1 |f this is a spam email
0 'email_ham44 |f this is a ham email</pre>
<p>This, to VW, is the same as:</p>
<pre>1 'email_spam1 |f this:1 is:1 a:1 spam:1 email:1
0 'email_ham44 |f this:1 is:1 a:1 ham:1 email:1</pre>
<p>LibSVM format demands numerical features, which requires a dictionary-&gt;key lookup. VW uses the <a href="http://en.wikipedia.org/wiki/Feature_hashing#Feature_vectorization_using_the_hashing_trick">hashing-trick</a>, making this more memory-friendly.</p>
<p>One could try <a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF*IDF</a> on these features, which would probably improve the score a bit, but that kinda defeats the purpose of <a href="http://en.wikipedia.org/wiki/Online_machine_learning">online machine learning</a>. Vowpal Wabbit does ok with binary vectors anyway.</p>
<h3>Results</h3>
<p>Running VW on the train set and generating predictions for the test set generated a top 1 score. Using an .eml reader I got to understand my datasets better. I noticed that a few <a href="http://en.wikipedia.org/wiki/Anti-spam_techniques#Rule-based_filtering">hand-written static rules</a> would be beneficial:</p>
<ul>
<li>If the sender has been messing with the sender field, for example by blanking it out, then the mail is likely spam.</li>
<li>If the date of the email is in the future (&#8220;3601-12-08&#8243;), then it is likely spam.</li>
<li>If the email contains the 2-gram: &#8220;Dear Friend&#8221;, then it is likely spam.</li>
<li>If the email is in all-caps then it is likely spam.</li>
<li>If the email contains spam reports from other classifiers (like <a href="http://spamassassin.apache.org/">SpamAssassin</a>), then these reports contains very indicative data on the mail being spam or not.</li>
</ul>
<p>The last rule helped the most. It is a bit weird that the train and test dataset contained these reports, as with a little parsing you can make your filter behave as good as commercial filters. But not all emails had these reports, or were scanned by a filter.</p>
<p>Using VW and the rule-based filter got me very close to 99%.</p>
<h3>Data engineering</h3>
<p>Since this contest had no rule on using external data, I finally got to try my hand at &#8220;data engineering&#8221;. Finding datasets to add to your model is a useful skill to have, and requires creativity, much like feature engineering does. Most main Kaggle contests explicitly forbid the usage of external data though, and probably for good reasons.</p>
<p>I exported my own GMail spam box and inbox to add to the datasets. I downloaded spam and ham datasets where I could find them. I managed to increase the train set by 1000%. But when I ran my deduplication scripts I found out to my surprise that I now had duplicate emails in train and test set&#8230; Turns out that the test set was generated from <a href="http://www.csmining.org/index.php/spam-email-datasets-.html">a dataset</a> (from 2010) that I had found too.</p>
<p>As this would definitely increase my score, and it would be unfair, I deleted those emails from my train set. This is a dangerous mistake to make, yet I am glad I made it and caught it on time. I wonder how much of a problem this is with academic benchmarks. As the spam-fighting world is a small world, many datasets may have some overlap. Then it matters to which degree algorithms can detect these duplicates.</p>
<p>But even with an all-unique non-test train set I increased the score to 99.89%. This shows that more training data is usually better. Vowpal Wabbit isn&#8217;t scared of this extra data: it is build to be scalable to millions of samples and thousands of features.</p>
<h3>Hitting the plateau</h3>
<p>I couldn&#8217;t increase the score much further. It seemed I had hit my theoretical limit of catching spam. A 100% classification accuracy would not be possible. I am not convinced the ground truth of the test set is even 100% correct.</p>
<p>Further small attempts to improve this score were fruitless, but it had proven to be enough to win this competition.</p>
<h3>What I learned</h3>
<ul>
<li>More training data is usually better</li>
<li>Do not create leakage for yourself, even if it may improve the score</li>
<li>Vowpal Wabbit eats spam for breakfast</li>
<li>Always try a quick Vowpal Wabbit run on classification and regression contests</li>
<li>It is really hard to beat a plateau at 99.9% (<a href="http://www.merl.com/publications/docs/TR2004-091.pdf">The Spam-Filtering Accuracy Plateau at 99.9 percent Accuracy and How to Get Past it</a>)</li>
</ul>
<h2>Papirusy z Edhellond (Papyrus scrolls from Edhellond)</h2>
<p><img class="alignleft size-full wp-image-445" alt="Kaggle Contest badge" src="http://mlwave.com/wp-content/uploads/2014/06/papirusy-z-edhellond.png" width="195" height="100" />For this competition I had to rely on Google Translate, as the <a href="http://inclass.kaggle.com/c/papirusy-z-edhellond">competition page</a> was written in Polish. This added a little to the complexity of the otherwise basic challenge.</p>
<p>For this competition the features were already generated and we didn&#8217;t have access to the raw original emails. We have features like:</p>
<ul>
<li>capital_run_length_total</li>
<li>char_freq_!</li>
<li>word_freq_credit</li>
</ul>
<p>for 3221 emails in train set. And we have to predict 1380 emails in test set. The evaluation metric is <a href="https://www.kaggle.com/wiki/AreaUnderCurve">Area under ROC</a>.</p>
<h3>Results</h3>
<p>I first tried Vowpal Wabbit. This was not enough to beat the leaders on leaderboard. A single research team had a very high score and over 200 submissions. I suspected they were overfitting to the leaderboard, and this was later confirmed, as they dropped to rank 3 when private leaderboard was revealed.</p>
<p>I switched to <a href="http://scikit-learn.org/">Scikit-learn</a>, focusing on their &#8220;ensemble&#8221; algo&#8217;s: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">RandomForestClassifier</a>, <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html">ExtraTreesClassifier</a> and <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">GradientBoostingClassifier</a>.</p>
<p>All of these produced very high scores. ExtraTreesClassifier did well on local Cross-Validation, but worse on the public leaderboard, so I mistakenly settled on RandomForestClassifier.</p>
<p>I tried all the different parameters using a <a href="http://scikit-learn.org/stable/modules/grid_search.html">gridsearch</a>, finding that &#8220;entropy&#8221; criteria split outperformed &#8220;gini&#8221; criteria split. I also tried <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html">AdaBoosting</a> large RandomForests, which upped the score a little.</p>
<p>I then tried two ensemble methods to increase the score even more: Simple <a href="http://en.wikipedia.org/wiki/Ensemble_averaging">averaging</a>/bagging my 3 best submissions. And a new technique for me called <a href="http://en.wikipedia.org/wiki/Ensemble_learning#Stacking">Stacked Ensemble Learning</a>: This performed really well using 6 small trees with different settings.</p>
<p>When the leaderboard was revealed my best submission turned out to be an ExtraTreesClassifier, second best was stacked ensemble learning. I had picked the Adaboosted RandomForestClassifier as my model which, while not the best possible score, did gave me the number one spot. See here my private and public leaderboard <a href="http://inclass.kaggle.com/c/papirusy-z-edhellond/forums/t/8203/question-to-the-competition-winner/44801#post44801">scores for different algo&#8217;s and different settings</a>.</p>
<h3>What I learned</h3>
<ul>
<li>Adaboosting large trees can increase the score</li>
<li>It is not impossible to compete in a contest that is written in another language.</li>
<li>Trust local Cross-validation, even when leaderboard tells you differently</li>
<li>Trust and hone instincts: if you think a model is more robust, but ranks lower than other models on public leaderboard, do not discard it.</li>
<li>Random Forests are very powerful and you should add them to your arsenal if you want to win Kaggle competitions</li>
<li>Ensemble learning can make the difference between winning a competition and doing well in a competition.</li>
<li>Simple bagging works, but stacking/blending usually works better.</li>
<li>6 weak models can beat 1 strong model</li>
</ul>
<h3>Code</h3>
<p>I have uploaded the <a href="https://github.com/MLWave/Kaggle-Papirusy-z-Edhellond">winning model code to Github</a>. Feel free to snoop around in it or use it improve the final score. I suspect that Adaboosted ExtraTrees will increase the score even more.</p>
<h2>Conclusion</h2>
<p><a href="http://inclass.kaggle.com/">Kaggle In Class contests</a> are fun to do, the problems are mostly entry-level, and these contests allow for a maximum of experimentation and learning. If you are allowed to join one, try it out some time. <a href="http://inclass.kaggle.com/c/model-t4">This competition</a> has you predict CPU load on a server cluster (See also this related Google blogpost: <a href="http://googleblog.blogspot.nl/2014/05/better-data-centers-through-machine.html">Better data centers through machine learning</a>).</p>
<p><small>The intro image to this post came from Wikimedia Commons and was created by <a href="http://www.bodenseepeter.de/">Peter Eich</a>.</small></p>
]]></content:encoded>
			<wfw:commentRss>http://mlwave.com/winning-2-kaggle-in-class-competitions-on-spam/feed/</wfw:commentRss>
		<slash:comments>6</slash:comments>
		</item>
	</channel>
</rss>
